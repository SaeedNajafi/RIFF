% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{comment}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Language Model-Friendly Paraphrase Search \\ for Robust Prompt Optimization}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Saeed Najafi \and
  Alona Fyshe \\
  Department of Computing Science, University of Alberta, Canada\\
  \texttt{\{snajafi,alona\}@ualberta.ca} \\}

\begin{document}
\maketitle
\begin{abstract}
Pre-trained Language Models (PLMs) are delivering surprising performance when they are fine-tuned on downstream text processing tasks. Recently, researchers efficiently guide these truly-big models to generate the desired output of a downstream task by optimizing the input prompts. Therefore, the prompt engineering research has solely focused on searching for the prompts that can be fed into the PLMs along with the original input text. Here, we investigate whether small modifications of the original input text would impact the final performance of the downstream task for various discrete and continuous prompt optimization techniques. Interestingly, our experiments in the few-shot Natural Language Understanding (NLU) datasets reveal that augmenting the training datasets with input paraphrases significantly boosts the performance of various prompt optimization techniques. To search these paraphrases which are helpful for the downstream language model, we propose a new learning objective based on maximum-marginal likelihood estimation and off-policy sampling. We also compare various perspectives in our objective design comparing with recent proximal policy optimization using different text decoding strategies. [talk about results for NLU in summary]
\end{abstract}


\section{Introduction}
Motivating points for introduction?

- Are LMs and the prompts or the adaptation matrices robust to paraphrasing inputs?

- What kind of modifications we can make?

- Read about behavioural testing of NLP models!

- Drop irrelevent phrases such as time phrases!

- Change named entities!

- Change the input tense!

- Generate paraphrases from a supervised model!

\section{Methods}

We focus on Natural Language Understanding (NLU) tasks where we have access to supervised training examples $D_{supp} = \{(x_i, y_i)\}^{i=N}_{i=1}$. We wish to update the parameter set $\theta_{lm}$ by maximizing the probability of the class label $y_i$ given the input $x_i$: $P_{\theta_{lm}} (y_i | x_i)$. To augment $D_{supp}$ with semi-supervised examples, we generate $M$ paraphrases for each $x_i$ using the paraphrase generator $P_{\theta_{par}} (z_{i,j} | x_{i})$ where $z_{i,j}$ is the $j$'s paraphrase for the input $x_{i}$ keeping the same semantic meaning with a different syntactic form. With these generated paraphrases, we are optimizing the following objective:
\begin{multline}
    J_{\theta_{lm}} := \sum^{N}_{i=1} \{ \log P_{\theta_{lm}} (y_i | x_i) + \\
    \sum^{M}_{j=1} P_{\theta_{par}} (z_{i,j} | x_{i}) \times \log P_{\theta_{lm}} (y_i | z_{i,j})\}
\label{lmfp-augmentation-objective}
\end{multline}  

The paraphrase examples are weighted by the paraphrase model $P_{\theta_{par}}$ which is fixed during optimizing the weights $\theta_{lm}$ of the language model.

\subsection{LM Tuning Techniques}
To train the language model optimizing the objective \ref{lmfp-augmentation-objective}, we need to adjust the set of parameters $\theta_{lm}$. Naively, we can update every parameter defined in the language model to optimize the training objective, but this ``All-Finetuning'' (AlTune) approach is not efficient if we do not have enough compute resource, especially with truly large language models. Therefore, we will study the effect of paraphrase augmentation along seven other efficient tuning~\cite{pmlr-v97-houlsby19a} or prompt engineering techniques~\cite{liu2021pretrain}. 

We assume that every input $x$ or its paraphrase $z_j$ has been prepended with a task instruction $p$. We represent task instruction with the symbol $p$ to be consistent with the prompt engineering literature. Adding the task instruction to the input can be interpreted as a parameter-free, gradient-free prompt engineering technique which uses the manually designed instruction to boost the performance of the pre-trained language model on various downstream tasks~\cite{DBLP:journals/corr/abs-2005-14165, petroni-etal-2019-language, deng-etal-2022-rlprompt}. With only the task instructions, we don't update any parameter of the language model $\theta_{lm}=\emptyset$ and we only make a zero-shot prediction on the development data. By augmenting the input or paraphrases with these task instructions, we further study the following LM tuning techniques:
\begin{itemize}
    \item Gradient-Search (GS): The GS technique is based on the recent AUTOPROMPT~\cite{shin-etal-2020-autoprompt} method which can optimize our task instructions without introducing any new parameters. The search starts in the vocabulary space by approximating the change in the label log-likelihood if we had replaced the token $p_{i}$ in the task instruction with another token $v$ from the vocabulary set. The search iteration in our implementation randomly picks a mini-batch of the training examples and then randomly picks one of tokens of the task instruction to update. It then finds the top $k$ new candidate tokens based on the change in the label log-likelihood approximated by the label directional gradient in the embedding direction $w_v$ of the candidate token $v$: $Top_v \; \{w^{T}_{v} \nabla_{w_{p_i}} \log P(y|p,x)$\}. The resulting new $k$ task instructions are compared again on the same training examples based on the label log-likelihood and the top new instruction is kept for the next search iteration. The search continues to the next iteration over another training mini-batch. We also monitor the task performance on the validation set and re-load the best saved task instruction if the current searched task instruction is not boosting the validation performance. This technique searches for an optimized task instruction in the discrete space and the resulting instructions are usually ungrammatical strings~\cite{shin-etal-2020-autoprompt}. The previous research applying this technique for prompt engineering always use the original input $x$ to search for the new task prompts~\cite{shin-etal-2020-autoprompt, deng-etal-2022-rlprompt}. We wish to investigate the effect of introducing paraphrases of $x$ during the search iterations.
    
    \item GrIPS: The Gradient-free Edit-based Instruction Search (GrIPS)~\cite{prasad-etal-2023-grips} is our second parameter-free search method for discrete prompt optimization. Similar to our previous GradientSearch implementation, within each search iteration over a training mini-batch, we try to perform a chain of four edit operations: deleting one token at random from the instruction, adding one of the previously deleted tokens back to the instruction at random, swapping two tokens in the instruction at random, and finally paraphrasing one of the tokens in the instruction using a pre-trained Pegasus paraphrase model. To evaluate the candidate instructions, GrIPS considers a score function based on the balanced accuracy augmented with the entropy of the predictions over the training mini-batch instead of the change in the label log-likelihood~\cite{prasad-etal-2023-grips}. We also monitor the task performance on the development set to save the best instruction out of the new candidate instructions similar to the GradientSearch method. The optimized instructions with GrIPS also tend to be ungrammatical strings. The paraphrase model used in GrIPS has only been used over the instructions and not the original text $x$. We investigate if further paraphrasing $x$ would improve the performance of GrIPS to find instructions that can generalize better.
    \item Input-Finetuning (InTune): As our simple efficient tuning technique, we only update the input embedding table in the transformer architecture. This method requires gradient computation similar to All-Finetuning (AlTune) (i.e. updating every parameter) and the GS method. The InTune method requires $\Theta(V \times D)$ parameters to update where $D$ is the dimension of the embedding vectors and $V$ is the vocabulary size~\footnote{we also update the bias and weight of the normalization layer associated with the input embedding table.}.
    \item LM-Head-Finetuning (HTune): The pre-trained language models based on the transformer architecture include a language modelling table (head) that maps the hidden vectors to the token logit (i.e. score before softmax function) per token of the vocabulary. We only update this language modelling head which requires $\Theta(D \times V)$ parameters where $D$ is the dimension of the hidden vectors of the final layer of the transformer architecture and $V$ is the vocabulary size.
    \item Classifier-Finetuning (ClsTune): We can use the averaged hidden vector $h(x)$ pooled from the final hidden vectors of the last layer in the language model as the feature representation for a given input $x$. In the Classifier-Finetuning (ClsTune), we assume our language model (feature extractor) is fixed and we only build a two-layer feedforward layer with the $gelu$ activation function~\cite{DBLP:journals/corr/HendrycksG16} as our classification module on top of the language model: $y = softmax(W \times gleu(U \times h(x) + f) + b)$. With ClsTune, we only update the weight matrices $W$ and $U$ as well as the bias vectors $f$ and $b$. We need to update $\Theta(D^{'} \times D + D^{'} + C \times D^{'} + C)$ parameters where $D$ is the dimension of the hidden vectors in the language model, $D^{'}$ is the corresponding dimension of the bias vector $f$, and $C$ is the number of labels. Despite HTune which generates the tokens corresponding to a class label, ClsTune classifies the input $x$ into all possible class ids. On the test data, ClsTune chooses the class with  highest logit, whereas HTune computes the probability for each possible class label and chooses the class with the most probable output.
    \item Softprompt-Tuning (SpTune): 
    \item LoRA-Adapting (LoAdapt):
\end{itemize}

\subsection{LM-Friendly Paraphrase Search}
In this section, we describe the learning objectives for training our paraphrase generator. Given a training example $(x, y)$, our goal is to classify the input $x$ into the gold label $y$ by maximizing the log likelihood $\log P(y|x)$. Our assumption is that there are some paraphrases of the input $x$ that could be easier for predicting the correct class. These paraphrases should maintain the semantic meaning of $x$ while being syntactically different from $x$. We wish to generate these paraphrases $z_{j}$ given the input $x$ such that the downstream language model can predict the correct label $y$ with higher confidence. Therefore, our data log likelihood is factorized into the following marginalization over paraphrases where $\theta_{par}$ and $\theta_{lm}$ are respectively the parameters of the paraphrase generator and the downstream language model:
\begin{multline}
J_{\theta_{par}} := \log E_{z_{j}} [P(y | z_{j})] = \\ \log \sum_{z_{j}} P_{\theta_{par}}(z_{j} | x) \times P_{\theta_{lm}}(y | z_{j})
\label{lmfp-main-objective}
\end{multline}


To train the paraphrase generator optimizing the objective \ref{lmfp-main-objective}, we consider four different learning perspectives: a) two techniques for gradient approximation, b) one reward scaling technique, c) three decoding techniques to sample paraphrases, and d) two techniques for maintaining grammar while generating paraphrases. These combinations result in different learning methods to fine-tune the paraphrase generator using the downstream language model. We describe our options for each perspective in the following paragraphs.

Given the $j$'s paraphrase $z_{j}$ for the training example $(x, y)$, we define the $z_{j}$'s reward as $R(z_{j}) = \log P_{\theta_{lm}} (y | z_{j})$.
To approximate the gradient vector of the objective \ref{lmfp-main-objective} with respect to $\theta_{par}$, we consider two options. We can use Maximum Marginal Likelihood (MML) estimation, or approximate the gradient vector for the paraphrase model using the Policy Gradient (PG) theorem. The gradient updates using these two approaches are closely related and they are only different in the posterior coefficient we use to score each sample \cite{guu-etal-2017-language}. We can re-formulate the main objective \ref{lmfp-main-objective} into the following expected reward:
\begin{multline}
J_{\theta_{par}} 
:= \log E_{z_{j}} [e^{R(z_{j})}],
z_{j} \sim P_{\theta_{par}}(.|x)
\label{lmfp-expect-objective}
\end{multline}

If for each input $x$, we take paraphrase samples from $P_{\theta_{par}}(.|x)$ and then approximate the expectation in $J_{\theta_{par}}$ with numerical summation, we are optimizing the objective using Maximum Marginal Likelihood estimation resulting in the following gradient update:
\begin{multline}
\nabla J^{mml}_{\theta_{par}} :=  \nabla_{\theta_{par}} \log E_{z_{j}} [e^{R(z_{j})}] = \\
\sum^{M}_{j=1} \phi^{mml}(z_{j}) \times \nabla_{\theta_{par}} \log P_{\theta_{par}}(z_{j}|x) \\
\phi^{mml}(z_{j}) = \frac{P_{\theta_{par}}(z_{j}|x) \times e^{R(z_{j})}}{\sum^{M}_{j^{'}=1} P_{\theta_{par}}(z_{j^{'}}|x) \times e^{R(z_{j^{'}})}}
\label{mml-objective}
\end{multline}

If we push the $\log$ inside the expectation (using Jensen's inequality), we can optimize a surrogate lower bound for the objective \ref{lmfp-expect-objective} resulting in the following policy gradient approximation \cite{10.5555/3009657.3009806}:
\begin{multline}
\nabla J^{pg}_{\theta_{par}} := \nabla_{\theta_{par}} E_{z_{j}} [R(z_{j})] = \\
\sum^{M}_{j=1} \phi^{pg}(z_{j}) \times \nabla_{\theta_{par}} \log P_{\theta_{par}}(z_{j}|x) \\
\phi^{pg}(z_{j}) = P_{\theta_{par}}(z_{j}|x) \times R(z_{j})
\label{pg-objective}
\end{multline}

For our second learning perspective, we can use the basic reward $R(z_{j})$ or normalize the rewards of the paraphrases for the same input $x$. This normalization helps to avoid training the paraphrase generator with rewards having different magnitudes corresponding to various input examples as some training pairs could be easier for the language model. Similar reward normalization has been reported to boost the performance of text generators across many tasks \cite{guo-etal-2022-efficient}. The normalized reward is defined as follows:
\begin{multline}
R^{n}(z_{j}) = \frac{R(z_{j}) - \mu_{j}}{\sigma_{j}}, \mu_{j} = \frac{1}{M} \sum^{M}_{j=1} R(z_{j}) \\
\sigma^{2}_{j} = \frac{1}{M} \sum^{M}_{j=1} (R(z_{j}) - \mu_{j})^2
\label{normal-reward}
\end{multline}

To train the paraphrase generator using the both the MML and PG objectives outlined in the equations \ref{mml-objective} and \ref{pg-objective}, we need to take $M$ samples from the paraphrase generator. We consider three decoding techniques to collect these samples. We first use beam search decoding to collect these $M$ paraphrases. To properly explore the paraphrase space, we can also take the $M$ paraphrases using nucleus (top-p) sampling~\cite{holtzman2020curious}. With the top-p sampling, we use the sampling threshold of $p=0.99$ where we collect the minimum set of tokens from the vocabulary set with the cumulative probability of at least $0.99$ and then re-sample tokens from this set. We also smooth the softmax logits for the tokens using the temperature value $t=2$ before applying the top-p sampling: $\frac{e^{score(v_i)/t}}{\sum_{v} e^{score(v)/t}}$. This smoothing helps to include more tokens in the sampling set~\footnote{Further smoothing $t>2$ results in degenerate paraphrases using the pre-trained paraphrase generator!}. As our third option for sampling during the training phase, we mix beam search and top-p sampling where we first sample $M$ paraphrases using the beam search and top-p samplings, and then mix the top $M/2$ samples from each of these outputs to form our final $M$ samples. To take samples for data augmentation (i.e. test phase) in the objective \ref{lmfp-augmentation-objective}, we only use top-p sampling without smoothing the softmax logits (i.e. $t=1$). As suggested by the previous work, top-p sampling can generate more diverse paraphrases using our pre-trained paraphrase generator~\cite{xu-etal-2020-autoqa}.

As our final learning perspective, we investigate whether the paraphrase generator starts to generate ungrammatical paraphrases or not. As we are sampling paraphrases from $P_{\theta_{par}}(z_{j}|x)$ and update the same set of parameters using these samples, the paraphrase generator can start generating ungrammatical text during this on-policy learning. Similar degenerate generation has been reported for question generation~\cite{najafi-fyshe-2023-weakly} or program synthesis~\cite{NEURIPS2018_f4e369c0}.

We experiment with two remedies for this degenerate generation. Similar to previous work on question generation~\cite{najafi-fyshe-2023-weakly}, we consider off-policy learning of the paraphrase generator where there is a fixed sampling module $P_{fixed}(z_{j}|x)$ to pick the samples and then we update the main paraphrase generator $P_{\theta_{par}}(z_{j}|x)$ within the objectives \ref{mml-objective} and \ref{pg-objective}. Therefore with these off-policy samples, the posterior coefficients include the importance sampling ratio $s(z_{j}) = \frac{P_{\theta_{par}}(z_{j}|x)}{P_{fixed}(z_{j}|x)}$:
\begin{multline}
\phi^{pg}_{off}(z_{j}) = s(z_{j}) \times R(z_{j})\\
\phi^{mml}_{off}(z_{j}) = \frac{s(z_{j}) \times e^{R(z_{j})}}{\sum^{M}_{j^{'}=1} s(z_{j^{'}}) \times e^{R(z_{j^{'}})}}
\label{off-pg-mml-objective}
\end{multline}

Our second solution for avoiding these degenerate paraphrases is to put a penalty in the training objective if the samples drawn from the current paraphrase generator $P_{\theta_{par}}(z_{j}|x)$ deviates a lot from the pre-trained paraphrase generator. This penalty could be implemented as a $KL$ penalty between the distributions of the paraphrases given by the current model and the pre-trained one. This penalty resembles the Proximal Policy Optimization (PPO) with the $KL$ penalty~\cite{DBLP:journals/corr/SchulmanWDRK17} and has also been used for fine-tuning of InstructGPT with the reward model trained over huma n feedback~\cite{ouyang2022training}. In the InstructGPT, the reward fine-tuned model cannot diverge from the language model pre-trained on the supervised data~\cite{ouyang2022training}. With this penalty, we define the following new objective for $\theta_{par}$:
\begin{multline}
J^{ppo}_{\theta_{par}} 
:= \log E_{z_{j}} [e^{R(z_{j})}] - \beta E_{z_{j}} [\log s(z_j)] \\
s(z_{j}) = \frac{P_{\theta_{par}}(z_{j}|x)}{P_{fixed}
(z_{j}|x)}, z_{j} \sim P_{\theta_{par}}(.|x)
\label{lmfp-expect-ppo-objective}
\end{multline}

Using the previous MML and PG gradient approximations, we have the following regularized gradient vectors with respect to $\theta_{par}$ ($\beta$ is a hyper-parameter):
\begin{multline}
\nabla J^{mml}_{\theta_{par}} - \beta E_{z_{j}} [(\log s(z_{j}) + 1) \nabla \log P_{\theta_{par}} (z_{j} | x)] \\
\nabla J^{pg}_{\theta_{par}} - \beta E_{z_{j}} [(\log s(z_{j}) + 1) \nabla \log P_{\theta_{par}} (z_{j} | x)] \\
z_{j} \sim P_{\theta_{par}}(.|x)
\label{lmfp-expect-ppo-gradient}
\end{multline}

It is worth noting that the KL penalty could be interpreted as a grammar reward given by $P_{fixed}(z_{j}|x)$ and an entropy regularization for $P_{\theta_{par}} (z_{j} | x)$. The entropy regularization helps with diverse exploration of the search space~\cite{DBLP:journals/corr/MnihBMGLHSK16} and the grammar reward hinders learning of the ungrammatical samples.

\subsection{Ensemble Inference}

\section{Experiments}

\subsection{Pre-trained Models}
As our pre-trained paraphrase model $P_{\theta_{par}} (z_{i,j} | x_{i})$, we use the BART-large \cite{lewis-etal-2020-bart} finetuned on the ParaBank2 dataset over 5 million sentence-paraphrase pairs \cite{hu-etal-2019-large}. These pairs have been constructed by back-translating the Czech portion of an English-Czech parallel corpus \cite{hu-etal-2019-large}. The model has been pre-trained with only token-level cross-entropy loss calculated using the gold paraphrase output given the input sentence.  We download the publically available\footnote{\url{https://huggingface.co/stanford-oval/paraphraser-bart-large}} weights trained as part of the AutoQA system \cite{xu-etal-2020-autoqa}. It is worth noting that our goal is not to train the state-of-the-art paraphrase generator as we study the effect of paraphrasing on prompt optimization techniques. Recent techniques for generating diverse paraphrases \cite{zhou-bhat-2021-paraphrase} could boost the performance in all our experiments. As some of our classification experiments have been conducted on the GLUE benchmark \cite{DBLP:journals/corr/abs-1804-07461}, we choose the BART-based model rather than the alternative T5 pre-trained model. The GLUE dataset has been part of the supervised corpora used for pre-training T5 models \cite{DBLP:journals/corr/abs-1910-10683}. The BART-large model finetuned on the paraphrase corpus has not observed the input sentences and the corresponding class labels of the GLUE datasets as part of its initial pre-training dataset.

As our main language model, we use the RoBERTa-large model pre-trained with Masked Language Modeling (MLM) objective \cite{DBLP:journals/corr/abs-1907-11692} which has been shown to perform well on Natural Language Understanding (NLU) tasks.

\subsection{Hyper-Parameters}

\subsection{Metrics}
- Describe the evaluation metrics for each task in superglue.

- Summarize the metric for sentiment analysis and sst-5.

\begin{comment}
\begin{table*}
\centering
\caption{The average accuracy (standard deviation) on the dev set of the SST2 binary sentiment classification task with the Roberta-Large LM trained in the fewshot setting using only 16 train examples per label. The results are averaged after sampling 32 examples for the train and validation splits using five different random seeds.$\dagger$ is the reported performance of the fine-tuned Roberta-Large on all of the training dataset.}

\begin{tabular}{c | c | c | c | c | c | c}
\hline
LM Tuning Method & Normal Train Data & +AUG & +AUG$_{avg}$ & +PAR & +PAR$_{avg}$ \\
\hline
\cite{DBLP:journals/corr/abs-1907-11692}$\dagger$ & 96.4 & n/a & n/a & n/a & n/a\\
\hline
%no instruction - no LM train & 77.2 & 77.2 & 77.2 & \\
with instruction - no LM train & 86.8 & 86.8 & 83.1 & 86.8 & 83.1\\
\hline
all\_finetune & 91.3 \small$\pm$\small1.2 & 91.0 \small$\pm$\small1.6 & 91.1 \small$\pm$\small1.6 & {\color{green}91.7} \small$\pm$\small1.3 & {\color{green}92.0} \small$\pm$\small1.6\\
input\_finetune & 89.7 \small$\pm$\small0.5 & {\color{green}91.1} \small$\pm$\small0.6 & {\color{green}91.4} \small$\pm$\small0.3 & {\color{green}90.2} \small$\pm$\small0.8 & {\color{green}91.1} \small$\pm$\small1.0\\
output\_finetune & 87.2 \small$\pm$\small3.3 & 86.5 \small$\pm$\small2.5 & 86.5 \small$\pm$\small2.8 & 86.1 \small$\pm$\small2.6 & 86.4 \small$\pm$\small2.4\\
classifier\_finetune & 64.5 \small$\pm$\small2.7 &  64.9 \small$\pm$\small4.6 & 64.9 \small$\pm$\small5.6 & {\color{green}66.7} \small$\pm$\small2.3 & {\color{green}66.5} \small$\pm$\small2.5 \\
\hline
prompt tune & 79.9 \small$\pm$\small7.5 & {\color{green}88.3} \small$\pm$\small5.0 & {\color{green}88.9} \small$\pm$\small4.8 & {\color{green}88.6} \small$\pm$\small3.3 & {\color{green}88.8} \small$\pm$\small3.1\\
gradient search & 86.5 \small$\pm$\small2.9 & {\color{green}89.0} \small$\pm$\small1.5 & {\color{green}88.2} \small$\pm$\small1.7 &
\end{tabular}
\label{sst2}
\end{table*}


The main Training Step:

\begin{multline}
P(y|x) = 0.5 \times P_{LM_{\phi}}(y|x) + \\
0.5 \times \sum_{x^{'} \in P_{sam}} \frac{P_{\theta}(x^{'} | x)}{P_{sam}(x^{'} | x)} \times P_{LM_{\phi}}(y | x^{'})
\end{multline}

- One table about the number of parameters being updated with each method.

- show the tuned learning rates per model tuning method.

- Depict the picture on the training examples as we plot Dev accuracy for these different methods.

- Describe what weights you have been updating.

- Describe the input format for each model and what you are basically updating with these models.


\begin{table}
\centering
\begin{tabular}{lc}
\hline
\textbf{Command} & \textbf{Output}\\
\hline
\verb|{\"a}| & {\"a} \\
\verb|{\^e}| & {\^e} \\
\verb|{\`i}| & {\`i} \\
\verb|{\.I}| & {\.I} \\
\verb|{\o}| & {\o} \\
\verb|{\'u}| & {\'u}  \\
\verb|{\aa}| & {\aa}  \\\hline
\end{tabular}
\begin{tabular}{lc}
\hline
\textbf{Command} & \textbf{Output}\\
\hline
\verb|{\c c}| & {\c c} \\
\verb|{\u g}| & {\u g} \\
\verb|{\l}| & {\l} \\
\verb|{\~n}| & {\~n} \\
\verb|{\H o}| & {\H o} \\
\verb|{\v r}| & {\v r} \\
\verb|{\ss}| & {\ss} \\
\hline
\end{tabular}
\caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
\label{tab:accents}
\end{table}
\section{Preamble}
\begin{table*}
\centering
\begin{tabular}{lll}
\hline
\textbf{Output} & \textbf{natbib command} & \textbf{Old ACL-style command}\\
\hline
\citep{ct1965} & \verb|\citep| & \verb|\cite| \\
\citealp{ct1965} & \verb|\citealp| & no equivalent \\
\citet{ct1965} & \verb|\citet| & \verb|\newcite| \\
\citeyearpar{ct1965} & \verb|\citeyearpar| & \verb|\shortcite| \\
\citeposs{ct1965} & \verb|\citeposs| & no equivalent \\
\citep[FFT;][]{ct1965} &  \verb|\citep[FFT;][]| & no equivalent\\
\hline
\end{tabular}
\caption{\label{citation-guide}
Citation commands supported by the style file.
The style is based on the natbib package and supports all natbib citation commands.
It also supports commands defined in previous ACL style files for compatibility.
}
\end{table*}
The first line of the file must be
\begin{quote}
\begin{verbatim}
\documentclass[11pt]{article}
\end{verbatim}
\end{quote}
To load the style file in the review version:
\begin{quote}
\begin{verbatim}
\usepackage[review]{ACL2023}
\end{verbatim}
\end{quote}
For the final version, omit the \verb|review| option:
\begin{quote}
\begin{verbatim}
\usepackage{ACL2023}
\end{verbatim}
\end{quote}
To use Times Roman, put the following in the preamble:
\begin{quote}
\begin{verbatim}
\usepackage{times}
\end{verbatim}
\end{quote}
(Alternatives like txfonts or newtx are also acceptable.)
Please see the \LaTeX{} source of this document for comments on other packages that may be useful.
Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.
By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
\begin{quote}
\begin{verbatim}
\setlength\titlebox{<dim>}
\end{verbatim}
\end{quote}
where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

\section{Document Body}

\subsection{Footnotes}

Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

\subsection{Tables and figures}

See Table~\ref{tab:accents} for an example of a table and its caption.
\textbf{Do not override the default caption sizes.}

\subsection{Hyperlinks}

Users of older versions of \LaTeX{} may encounter the following error during compilation:
\begin{quote}
\tt\verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
\end{quote}
This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

\subsection{Citations}



Table~\ref{citation-guide} shows the syntax supported by the style files.
We encourage you to use the natbib styles.
You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

\end{comment}

\section{Related Works}

\noindent
{\bf Prompt Engineering}:
Describe P-tuning, P-Tuning V2, Prefix-Tuning, Blax-Box Tuning, Levenstian Tuning. Chain of Thought Prompting.

The recent discrete optimization method RLPrompt \cite{deng-etal-2022-rlprompt} which uses the on-policy version of the soft Q-learning \cite{https://doi.org/10.48550/arxiv.2106.07704} to find the optimal discrete prompt tokens in a gradient-free setting.

\noindent
{\bf Paraphrase Generation}:
Our main goal in this study is not to provide a state-of-the-art system for diverse paraphrase generation. We investigate the effect of input paraphrasing on the prompt optimization techniques for efficiently querying large language models.

\section{Limitations}
- We are not doing the best in paraphrasing, more can be done by deploying better paraphraser models.

- Merging prompts with original inputs.

- Tested on large models only, not test on truly large models.

- Pre-trained paraphrase generator requires a supervised corpus for that particular language. More research on unsupervised paraphrase generation maybe helpful.

\section*{Ethics Statement}
Scientific work published at ACL 2023 must comply with the ACL Ethics Policy.\footnote{\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

\subsection{References}

\nocite{Ando2005,augenstein-etal-2016-stance,andrew2007scalable,rasooli-tetrault-2015,goodman-etal-2016-noise,harper-2014-learning}

The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
\begin{quote}
\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{custom}
\end{verbatim}
\end{quote}
You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
To include both the Anthology and your own .bib file, use the following instead of the above.
\begin{quote}
\begin{verbatim}
\bibliographystyle{acl_natbib}
\bibliography{anthology,custom}
\end{verbatim}
\end{quote}
Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

\subsection{Appendices}

Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

\section{Bib\TeX{} Files}
\label{sec:bibtex}

Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

\section*{Acknowledgements}
This document has been adapted by Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers from the style files used for earlier ACL, EMNLP and NAACL proceedings, including those for
EACL 2023 by Isabelle Augenstein and Andreas Vlachos,
EMNLP 2022 by Yue Zhang, Ryan Cotterell and Lea Frermann,
ACL 2020 by Steven Bethard, Ryan Cotterell and Rui Yan,
ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell,
ACL 2012 by Maggie Li and Michael White,
ACL 2010 by Jing-Shin Chang and Philipp Koehn,
ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
ACL 2002 by Eugene Charniak and Dekang Lin,
and earlier ACL and EACL formats written by several people, including
John Chen, Henry S. Thompson and Donald Walker.
Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Example Appendix}
\label{sec:appendix}

This is a section in the appendix.

\end{document}
