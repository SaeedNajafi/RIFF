% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[final]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{comment}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Language Model-Friendly Paraphrase Search \\
for Robust Prompt Optimization}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Saeed Najafi \and
  Alona Fyshe \\
  Department of Computing Science, University of Alberta, Canada\\
  \texttt{\{snajafi,alona\}@ualberta.ca} \\}

\begin{document}
\maketitle
\begin{abstract}
Pre-trained Language Models (PLMs) have shown remarkable performance when fine-tuned on downstream text processing tasks. Recent research has efficiently guided these large models to generate desired outputs for specific downstream tasks by optimizing the input prompts. However, prompt engineering research has primarily focused on finding effective prompts that can be combined with the original task input text and fed into PLMs. In this study, we investigate whether making small modifications to the original task input text can impact the final performance of downstream tasks when used alongside various discrete and continuous prompt optimization techniques. Our experiments on few-shot Natural Language Understanding (NLU) datasets reveal that augmenting the training datasets with paraphrases of input text significantly enhances the prompt optimization performance. To find paraphrases that best enhance the downstream language model, we propose a new learning objective based on maximum-marginal likelihood (MML) estimation and off-policy sampling to tune the paraphrase model. We show that MML with off-policy sampling outperforms recently proposed proximal policy optimization method, considering different text decoding strategies such as beam search and top-p sampling.
% [Further details on the results for NLU tasks]
\end{abstract}


\section{Introduction}
Motivating points for introduction?

- Are LMs and the prompts or the adaptation matrices robust to paraphrasing inputs?

- What kind of modifications we can make?

- Drop irrelevent phrases such as time phrases!

- Change named entities!

- Change the input tense!

\section{Methods}
We focus on classification problem for Natural Language Understanding (NLU) tasks where we have access to supervised training examples $D_{\text{supp}} = \{(x_i, y_i)\}_{i=1}^{N}$. Our goal is to update the parameter set $\theta_{\text{lm}}$ of a language model by maximizing the probability of the class label $y_i$ given the input $x_i$: $P_{\theta_{\text{lm}}} (y_i | x_i)$. To augment $D_{\text{supp}}$ with semi-supervised examples, we generate $M$ paraphrases for each $x_i$ using the paraphrase generator $P_{\theta_{\text{par}}} (z_{i,j} | x_i)$, where $z_{i,j}$ represents the $j$-th paraphrase for the input $x_i$, preserving the same semantic meaning but with a different syntactic form. By incorporating these generated paraphrases, we optimize the following objective function:
\begin{multline}
J_{\theta_{\text{lm}}} := \sum_{i=1}^{N} \{\log P_{\theta_{\text{lm}}} (y_i | x_i) + \\
\sum_{j=1}^{M} P_{\theta_{\text{par}}} (z_{i,j} | x_i) \times \log P_{\theta_{\text{lm}}} (y_i | z_{i,j})\}
\label{lmfp-augmentation-objective}
\end{multline}

The generated weakly-supervised example $(z_{i,j}, y_i)$ is assigned a generation weight based on the paraphrase model $P_{\theta_{\text{par}}}$, which remains fixed while optimizing the weights $\theta_{\text{lm}}$ of the language model.

\subsection{LM Tuning Techniques}
In order to train the language model and optimize the objective \ref{lmfp-augmentation-objective}, adjustments need to be made to the parameter set $\theta_{\text{lm}}$. While a naive approach would involve updating every parameter in the language model to optimize the training objective (referred to as the "All-Finetuning" or AlTune approach), this method can be computationally inefficient, especially with large language models and limited computational resources. As a result, we will explore the impact of paraphrase augmentation along with seven other efficient tuning techniques~\cite{pmlr-v97-houlsby19a} or prompt engineering techniques~\cite{liu2021pretrain}.

We assume that each input $x$ or its paraphrase $z_j$ is preceded by a task instruction $p$. The task instruction, represented by the symbol $p$ for consistency with prompt engineering literature, serves as a parameter-free, gradient-free technique for enhancing the performance of the pre-trained language model across various downstream tasks~\cite{DBLP:journals/corr/abs-2005-14165, petroni-etal-2019-language, deng-etal-2022-rlprompt}. When using only the task instructions, no parameters of the language model are updated ($\theta_{\text{lm}}=\emptyset$), and zero-shot predictions are made solely on the development data. By incorporating these task instructions into the input or paraphrases, we further investigate the following language model tuning techniques:
\begin{itemize}

    \item Gradient-Search (GS): The GS technique is based on the recent AUTOPROMPT~\cite{shin-etal-2020-autoprompt} method, which allows for optimizing task instructions without introducing new parameters. The search process begins in the vocabulary space, approximating the change in label log-likelihood when replacing token $p_{i}$ in the task instruction with another token $v$ from the vocabulary set. In our implementation, each search iteration randomly selects only one mini-batch of training examples and then randomly selects a token from the task instruction to update. The top $k$ candidate tokens are determined based on the change in label log-likelihood, approximated using the label directional gradient in the embedding direction $w_v$ of the candidate token $v$: $Top_v \; \{w^{T}_{v} . \nabla_{w_{p_i}} \log P_{\text{lm}}(y|p,x)\}$. The resulting $k$ new task instructions are evaluated again on the same training examples\footnote{The original AUTOPROMPT evaluates the new candidate instructions on another training mini-batch. For fewshot classification, we re-use the drawn training mini-batch to evaluate the complete new candidate instructions.}, considering the label log-likelihood, and the top-performing instruction is retained for the next search iteration. This process continues with the next iteration using another training mini-batch. We also monitor the task performance on the validation set and revert to the best saved task instruction if the current search instruction does not enhance validation performance. It is worth noting that this technique finds an optimized task instruction in the discrete space, which often yields ungrammatical strings~\cite{shin-etal-2020-autoprompt}. Previous research applying this technique for prompt optimization always employed the original input $x$ for searching new task prompts~\cite{shin-etal-2020-autoprompt, deng-etal-2022-rlprompt}. However, we aim to investigate the impact of incorporating paraphrases of $x$ during the search iterations.
    
    
    \item GrIPS: The Gradient-free Edit-based Instruction Search (GrIPS)~\cite{prasad-etal-2023-grips} serves as our second parameter-free method for discrete prompt optimization. Similar to the previous GS technique, GrIPS conducts a search iteration over a training mini-batch. Within each iteration, a sequence of four edit operations is performed: randomly deleting one token from the instruction, randomly adding one of the previously deleted tokens back to the instruction, randomly swapping two tokens in the instruction, and finally paraphrasing one of the tokens in the instruction using a pre-trained Pegasus paraphrase model~\cite{pmlr-v119-zhang20ae}. In evaluating candidate instructions, GrIPS employs a score function based on balanced accuracy, augmented with the entropy of predictions over the training mini-batch, instead of measuring the change in label log-likelihood~\cite{prasad-etal-2023-grips}. Similar to the GS method, we monitor the task performance on the development set to retain the best instruction if the new instructions are not enhancing the performance. Optimized instructions generated by GrIPS also tend to be ungrammatical strings. It is worth noting that the paraphrase model used in GrIPS has only been applied to the instructions, not the original text $x$. Therefore, we explore whether further paraphrasing of $x$ would enhance the performance of GrIPS in discovering instructions that offer better generalization.

    \item Input-Finetuning (InTune): As a straightforward and efficient tuning technique, we only update the input embedding table in the transformer architecture. This method requires gradient computation similar to All-Finetuning (AlTune), which involves updating every parameter, as well as the GS method. During training with the InTune approach, $\Theta(V \times D)$ parameters are required for updating, where $D$ represents the dimension of the embedding vectors and $V$ denotes the vocabulary size~\footnote{Additionally, the bias and weight of the normalization layer associated with the input embedding table are also updated.}.
    
    \item  LM-Head-Finetuning (HTune): The transformer-based pre-trained language models consist of a language modeling head, which maps the hidden vectors to the token logit (i.e., the score before the softmax function) for each token in the vocabulary. In the HTune technique, we solely update this language modeling head. This process involves updating $\Theta(D \times V)$ parameters, where $D$ represents the dimension of the hidden vectors in the final layer of the transformer architecture, and $V$ denotes the vocabulary size.

    \item Classifier-Finetuning (ClsTune): In ClsTune, we first create a feature representation $h(x)$ for the input text $x$ using average pooling of the final hidden vectors in the last layer of the language model. Here, we assume that the language model (feature extractor) remains fixed, and we then construct a two-layer feedforward layer with the $gelu$ activation function~\cite{DBLP:journals/corr/HendrycksG16} as our classification module on top of the language model. The classification module is defined as follows: $y = softmax(W \times gelu(U \times h(x) + f) + b)$. In ClsTune, we only update the weight matrices $W$ and $U$, along with the bias vectors $f$ and $b$. The total number of parameters to update in ClsTune is $\Theta(D' \times D + D' + C \times D' + C)$, where $D$ represents the dimension of the hidden vectors in the language model, $D'$ corresponds to the dimension of the bias vector $f$, and $C$ indicates the number of labels. Unlike HTune, which generates tokens associated with a class label, ClsTune classifies the input $x$ into all possible class IDs. During testing, ClsTune selects the class with the highest logit, while HTune calculates the probability for each potential class label and selects the class with the most probable output.

    \item Softprompt-Tuning (SpTune): In SpTune~\cite{lester-etal-2021-power}, we employ a technique where $L$ virtual prompt tokens are prepended to the task instruction. These $L$ virtual tokens are associated with $L$ dedicated prompt embedding vectors, extending the sequence of vectors derived from the task instruction and input text with an additional $L$ trainable feature vectors. During training, the original embedding table of the transformer model remains fixed, while the prompt embedding table is trained by backpropagating the label log-likelihood into the prompt embedding table. This method necessitates the update of $\Theta(L \times D)$ parameters, where $D$ denotes the dimension of the embedding table in the language model, and $L$ signifies the length of the prompt. 
    
    \item Low-Rank Adaptation (LoRA): LoRA is one of the latest efficient-tuning techniques specifically designed for large language models~\cite{DBLP:journals/corr/abs-2106-09685}. It learns low-rank adaptation matrices for the query and value weight matrices within the transformer model. For a pre-trained weight matrix $W_q \in R^{d \times k}$, LoRA learns the necessary adaptation (i.e., modification) of the weight matrix for a downstream task through a low-rank decomposition, expressed as $W_q + \triangle W_q = W_q + BA$. Here, $B \in R^{d \times r}$, $A \in R^{r \times k}$, and the rank $r \le min(d, k)$. The adaptation matrices $A$ and $B$ are the only parameters subject to training, while the original matrix $W_q$ does not receive any gradient updates. Studies have shown that LoRA performs on par with, or better than, AlTune across various large language models~\cite{DBLP:journals/corr/abs-2106-09685}.
    
\end{itemize}

All the Language Model (LM) tuning techniques we have discussed will use the same input format. For example in the sentiment classification task, we use the following format:
\textbf{``<s> \{instruction\} \{text\} . It was <mask> </s>''}. Except for ClsTune, all of our LM tuning techniques aim to maximize the probability of the correct label token replacing the <mask> token. In contrast, ClsTune takes the formatted input and classifies it into one of the predefined class IDs.
\subsection{LM-Friendly Paraphrase Search}
In this section, we present the learning objectives for training our paraphrase generator. Given a training example $(x, y)$, our objective is to classify the input $x$ into the gold label $y$ by maximizing the log likelihood $\log P(y|x)$. We assume that there exist paraphrases of the input $x$ that could facilitate the prediction of the correct class. These paraphrases should retain the semantic meaning of $x$ while exhibiting syntactic differences. Our aim is to generate paraphrases $z_{j}$ based on the input $x$, enabling the downstream language model to predict the correct label $y$ with greater confidence. Consequently, our data log likelihood is factorized into the following marginalization over the space of paraphrases, where $\theta_{\text{par}}$ and $\theta_{\text{lm}}$ represent the parameters of the paraphrase generator and the downstream language model, respectively:
\begin{multline}
J_{\theta_{\text{par}}} := \log E_{z_{j}} [P(y | z_{j})] = \\ \log \sum_{z_{j}} P_{\theta_{\text{par}}}(z_{j} | x) \times P_{\theta_{\text{lm}}}(y | z_{j})
\label{lmfp-main-objective}
\end{multline}


To train the paraphrase generator and optimize the objective stated in Equation~\ref{lmfp-main-objective}, we explore four distinct learning aspects: a) two methods for approximating gradients, b) a reward normalization technique, c) three decoding techniques for sampling paraphrases, and d) two approaches to ensure grammatical integrity during paraphrase generation. By combining these elements, we employ various learning approaches to refine the paraphrase generator with the aid of the downstream language model. In the subsequent paragraphs, we will describe our suggested options for each aspect.

\subsubsection{Gradient Approximation}
For a given training example $(x, y)$ and its paraphrase $z_{j}$, we define the reward for $z_{j}$ as $R(z_{j}) = \log P_{\theta_{\text{lm}}} (y | z_{j})$.

When approximating the gradient vector of objective~\ref{lmfp-main-objective} concerning $\theta_{\text{par}}$, we propose two strategies. These include using Maximum Marginal Likelihood (MML) estimation or approximating the gradient vector of the paraphrase model via the Policy Gradient (PG) theorem. Notably, gradient updates using these two methods exhibit a close relationship, with the main difference lying in the posterior coefficient utilized to score each sample~\cite{guu-etal-2017-language}. We can recast the main objective~\ref{lmfp-main-objective} into the following function representing the expected reward:
\begin{multline}
J_{\theta_{\text{par}}}
:= \log E_{z_{j}} [e^{R(z_{j})}],
z_{j} \sim P_{\theta_{\text{par}}}(.|x)
\label{lmfp-expect-objective}
\end{multline}

Given each input $x$, if we extract paraphrase samples from $P_{\theta_{\text{par}}}(.|x)$ and approximate the expectation in $J_{\theta_{\text{par}}}$ via numerical summation, we optimize the objective using MML estimation. This process results in the following gradient update:
\begin{multline}
\nabla J^{\text{mml}}_{\theta_{\text{par}}} := \nabla_{\theta_{\text{par}}} \log E_{z_{j}} [e^{R(z_{j})}] = \\
\sum^{M}_{j=1} \phi^{\text{mml}}(z_{j}) \times \nabla_{\theta_{\text{par}}} \log P_{\theta_{\text{par}}}(z_{j}|x) \\
\phi^{\text{mml}}(z_{j}) = \frac{P_{\theta_{\text{par}}}(z_{j}|x) \times e^{R(z_{j})}}{\sum^{M}_{j^{'}=1} P_{\theta_{\text{par}}}(z_{j^{'}}|x) \times e^{R(z_{j^{'}})}}
\label{mml-objective}
\end{multline}

By introducing the $\log$ inside the expectation (applying Jensen's inequality), we can optimize a surrogate lower bound for the objective~\ref{lmfp-expect-objective}, resulting in the following policy gradient approximation~\cite{10.5555/3009657.3009806}:
\begin{multline}
\nabla J^{\text{pg}}_{\theta_{\text{par}}} := \nabla_{\theta_{\text{par}}} E_{z_{j}} [R(z_{j})] = \\
\sum^{M}_{j=1} \phi^{\text{pg}}(z_{j}) \times \nabla_{\theta_{\text{par}}} \log P_{\theta_{\text{par}}}(z_{j}|x) \\
\phi^{\text{pg}}(z_{j}) = P_{\theta_{\text{par}}}(z_{j}|x) \times R(z_{j})
\label{pg-objective}
\end{multline}

\subsubsection{Reward Normalization}
For our secondary learning perspective, we can either utilize the basic reward, denoted as $R(z_{j})$, or normalize the rewards among the paraphrases of a given input $x$. This process of normalization is particularly useful because it prevents the training of the paraphrase generator with rewards of varying magnitudes, which could correspond to different input examples. This situation may arise as some training pairs could be easier for the language model to handle. Previous reports suggest that such normalization of rewards can significantly enhance the performance of text generators across a variety of tasks \cite{guo-etal-2022-efficient}. The normalized reward is defined as follows:
\begin{multline}
R^{n}(z_{j}) = \frac{R(z_{j}) - \mu_{j}}{\sigma_{j}}, \mu_{j} = \frac{1}{M} \sum^{M}_{j=1} R(z_{j}) \\
\sigma^{2}_{j} = \frac{1}{M} \sum^{M}_{j=1} (R(z_{j}) - \mu_{j})^2
\label{normal-reward}
\end{multline}

\subsubsection{Decoding Techniques}
To train the paraphrase generator, we employ both the MML and PG objectives as outlined in equations \ref{mml-objective} and \ref{pg-objective}, which necessitates drawing $M$ samples from the paraphrase generator. We implement three decoding techniques for this purpose. Firstly, we utilize beam search decoding to gather these $M$ paraphrases. In order to thoroughly explore the paraphrase space, we alternatively collect the $M$ paraphrases using nucleus (top-p) sampling~\cite{holtzman2020curious}. For the top-p sampling, we establish a sampling threshold of $p=0.99$, at which we collect the minimal set of tokens from the vocabulary with a cumulative probability of at least $0.99$. We then re-sample tokens from this set. Prior to applying top-p sampling, we smooth the softmax logits for the tokens using a temperature value of $t=2$: $\frac{e^{score(v_i)/t}}{\sum_{v} e^{score(v)/t}}$. This smoothing helps incorporate more tokens into the sampling set~\footnote{Any further smoothing with $t>2$ results in degenerate paraphrases using the pre-trained paraphrase model!}. As a third option during the training phase, we blend beam search and top-p sampling. Here, we initially sample $M$ paraphrases using both methods, then combine the top $M/2$ samples from each output to construct our final $M$ samples. For data augmentation (i.e., during the test phase) in objective \ref{lmfp-augmentation-objective}, we solely use top-p sampling, and do not smooth the softmax logits (i.e., $t=1$). As indicated by previous research, top-p sampling has been found to generate more diverse paraphrases when using our pre-trained paraphrase generator~\cite{xu-etal-2020-autoqa}.

\subsubsection{Grammar Integrity}
In our final learning perspective, we aim to examine whether or not the paraphrase generator begins to produce ungrammatical paraphrases. As we are sampling paraphrases from $P_{\theta_{\text{par}}}(z_{j}|x)$ and updating the same set of parameters using these samples, the paraphrase generator may start generating ungrammatical text during this on-policy learning phase. Similar instances of degenerate generation have been reported in tasks like question generation~\cite{najafi-fyshe-2023-weakly} and program synthesis~\cite{NEURIPS2018_f4e369c0}.

To mitigate this degenerate generation, we experiment with two solutions. In line with previous work on question generation~\cite{najafi-fyshe-2023-weakly}, we consider off-policy learning for the paraphrase generator. Here, we maintain a fixed sampling module $P_{\text{fixed}}(z_{j}|x)$ for sample selection, then update the main paraphrase generator $P_{\theta_{\text{par}}}(z_{j}|x)$ within the frameworks of objectives \ref{mml-objective} and \ref{pg-objective}. Consequently, with these off-policy samples, the posterior coefficients incorporate the importance sampling ratio $s(z_{j}) = \frac{P_{\theta_{\text{par}}}(z_{j}|x)}{P_{\text{fixed}}(z_{j}|x)}$
\begin{multline}
\phi^{\text{pg}}_{\text{off}}(z_{j}) = s(z_{j}) \times R(z_{j})\\
\phi^{\text{mml}}_{\text{off}}(z_{j}) = \frac{s(z_{j}) \times e^{R(z_{j})}}{\sum^{M}_{j^{'}=1} s(z_{j^{'}}) \times e^{R(z_{j^{'}})}}
\label{off-pg-mml-objective}
\end{multline}

To avoid these degenerate paraphrases, our second solution involves imposing a penalty in the training objective if the samples drawn from the current paraphrase generator, $P_{\theta_{\text{par}}}(z_{j}|x)$, significantly deviate from those of the pre-trained paraphrase generator. We can implement this penalty as a KL-divergence penalty between the distributions of paraphrases produced by the current model and the pre-trained one. This approach resembles the Proximal Policy Optimization (PPO) with a KL penalty~\cite{DBLP:journals/corr/SchulmanWDRK17} and has been used in fine-tuning InstructGPT with a reward model trained over human feedback~\cite{ouyang2022training}. In InstructGPT's case, the reward fine-tuned model is prevented from diverging from the language model that's pre-trained on supervised data~\cite{ouyang2022training}. With this penalty in place, we define the following new objective for $\theta_{\text{par}}$:
\begin{multline}
J^{\text{ppo}}_{\theta_{\text{par}}} 
:= \log E_{z_{j}} [e^{R(z_{j})}] - \beta E_{z_{j}} [\log s(z_j)] \\
s(z_{j}) = \frac{P_{\theta_{\text{par}}}(z_{j}|x)}{P_{\text{fixed}}
(z_{j}|x)}, z_{j} \sim P_{\theta_{\text{par}}}(.|x)
\label{lmfp-expect-ppo-objective}
\end{multline}

Building upon the previously approximated MML and PG gradients, we can now derive the following regularized gradient vectors with respect to $\theta_{\text{par}}$. Please note that $\beta$ is a hyper-parameter in this context:
\begin{multline}
\nabla J^{\text{mml}}_{\theta_{\text{par}}} - \beta E_{z_{j}} [(\log s(z_{j}) + 1) \nabla \log P_{\theta_{\text{par}}} (z_{j} | x)] \\
\nabla J^{\text{pg}}_{\theta_{\text{par}}} - \beta E_{z_{j}} [(\log s(z_{j}) + 1) \nabla \log P_{\theta_{\text{par}}} (z_{j} | x)] \\
z_{j} \sim P_{\theta_{\text{par}}}(.|x)
\label{lmfp-expect-ppo-gradient}
\end{multline}

It's important to note that the KL penalty can be interpreted as the sum of a grammar reward, denoted by $\log P_{\text{fixed}}(z_{j}|x)$, and an entropy regularization term over $P_{\theta_{\text{par}}} (z_{j} | x)$. The entropy regularization aids in the diverse exploration of the search space~\cite{DBLP:journals/corr/MnihBMGLHSK16}, while the grammar reward discourages the learning of ungrammatical samples.

\subsection{Ensemble Inference}
After optimizing the objective \ref{lmfp-main-objective} and fine-tuning our paraphrase generator, we generate weakly-supervised examples for inclusion in the objective \ref{lmfp-augmentation-objective} to train our downstream language model.

To predict the label of a test example, we could either use our fine-tuned language model to predict the class based on the original input $x$, or adopt an ensembling approach. For the latter, for a given $x$, we generate $M$ paraphrases using our fine-tuned paraphrase generator. We then average the prediction scores for a potential class across the $M+1$ values to predict the class for that input example $x$. This aligns with our earlier assumption that some paraphrases could be easier for the language model to predict the correct label.
\section{Experiments}

\subsection{Setup}
\subsubsection{Pre-trained Models}
We employ BART-large~\cite{lewis-etal-2020-bart}, fine-tuned on the ParaBank2 dataset over 5 million sentence-paraphrase pairs~\cite{hu-etal-2019-large}, as our pre-trained paraphrase model, $P_{\theta_{\text{par}}} (z_{i,j} | x_{i})$. These pairs were constructed by back-translating the Czech portion of an English-Czech parallel corpus~\cite{hu-etal-2019-large}. The model has been pre-trained with a token-level cross-entropy loss, calculated using the gold paraphrase output from the input sentence. We use the publicly available weights\footnote{\url{https://huggingface.co/stanford-oval/paraphraser-bart-large}} that were trained as part of the AutoQA system~\cite{xu-etal-2020-autoqa}.

It is worth noting that our goal is not to train a state-of-the-art paraphrase generator, but to study the impact of paraphrasing on prompt optimization techniques. Recent techniques for generating diverse paraphrases~\cite{zhou-bhat-2021-paraphrase} could enhance performance in all our experiments. Given that some of our classification experiments were conducted on the GLUE benchmark~\cite{DBLP:journals/corr/abs-1804-07461}, we opted for the BART-based model over the T5 pre-trained model. This decision was made because the GLUE dataset was part of the supervised corpora used for pre-training T5 models~\cite{DBLP:journals/corr/abs-1910-10683}. Moreover, the BART-large model, fine-tuned on the paraphrase corpus, has not encountered the input sentences and corresponding class labels of the GLUE datasets in its initial pre-training dataset.

For our main language model, we utilize the RoBERTa-large model, pre-trained with the Masked Language Modeling (MLM) objective~\cite{DBLP:journals/corr/abs-1907-11692}, which has demonstrated strong performance on Natural Language Understanding (NLU) tasks. Our proposed learning framework can be readily extended to other paraphrase generators or backbone language models.

\subsubsection{Datasets}
Inspired by prior work~\cite{gao-etal-2021-making, deng-etal-2022-rlprompt}, we conduct several few-shot classification tasks as part of our experiments. These include sentiment classification tasks such as the binary sentiment dataset SST2~\cite{socher-etal-2013-recursive}, the 5-label sentiment dataset SST5~\cite{socher-etal-2013-recursive}, as well as the topic classification dataset AG's News~\cite{NIPS2015_250cf8b5}. Our experiments are carried out under two few-shot settings: $k$-shot with $k \in \{16, 128\}$, in which we randomly select $k$ training examples for each unique label within the dataset. An equal number of examples are gathered to form an internal validation set. We report results on the standard development split for each dataset, with averages taken across five different models trained over five randomly generated train/validation splits. The model delivering the best performance on the validation data is then used for prediction on the development set. The number of classes per dataset, as well as the instructions used for each, are outlined in Appendix~\ref{task-instruct-input-format:appendix}. Instructions and class verbalizers for the generative LM tuning techniques are based on the previous work~\cite{deng-etal-2022-rlprompt} in prompt optimization.
\subsubsection{Training \& Testing Details}
We conducted few-shot experiments using the seeds \{11, 42, 1993, 12321, 2023\}. The learning rate for each LM tuning technique was separately fine-tuned from the set \{0.5, 0.3, 0.1, 0.01, 0.001, 0.0001, 0.00001\} using the seed 11 from the 16-shot experiments on the SST2 dataset, and was then applied globally across other datasets and experiments. For paraphrase fine-tuning, we use the learning rate of 0.00001. Detailed information about the specific learning rates used for each LM technique, along with other hyperparameters we employed across all experiments and datasets, can be found in Appendix~\ref{training-details-extra:appendix}. For optimization, we utilized the AdamW~\cite{DBLP:journals/corr/abs-1711-05101}\footnote{\url{https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html}} optimizer with the AMSGrad variant set to True~\cite{DBLP:journals/corr/abs-1904-09237}. We implemented the methods using the HuggingFace\footnote{\url{https://huggingface.co/}} library and the PyTorch\footnote{\url{https://pytorch.org/}} machine learning framework. The experiments were conducted using multiple NVIDIA's A40 and RTX6000 GPU cards (upto 60 GPU cards). This was facilitated by the GPU cluster provided by the Vector Institute\footnote{\url{https://vectorinstitute.ai/}}. We report the accuracy metric on the classification datasets.

\subsection{Paraphrase Training}

\subsection{Data Augmentation}
- some generated paraphrases before tuning.
- some generated paraphrases after tuning.

\begin{comment}
\begin{table*}
\centering
\caption{The average accuracy (standard deviation) on the dev set of the SST2 binary sentiment classification task with the Roberta-Large LM trained in the fewshot setting using only 16 train examples per label. The results are averaged after sampling 32 examples for the train and validation splits using five different random seeds.$\dagger$ is the reported performance of the fine-tuned Roberta-Large on all of the training dataset.}

\begin{tabular}{c | c | c | c | c | c | c}
\hline
LM Tuning Method & Normal Train Data & +AUG & +AUG$_{avg}$ & +PAR & +PAR$_{avg}$ \\
\hline
\cite{DBLP:journals/corr/abs-1907-11692}$\dagger$ & 96.4 & n/a & n/a & n/a & n/a\\
\hline
%no instruction - no LM train & 77.2 & 77.2 & 77.2 & \\
with instruction - no LM train & 86.8 & 86.8 & 83.1 & 86.8 & 83.1\\
\hline
all\_finetune & 91.3 \small$\pm$\small1.2 & 91.0 \small$\pm$\small1.6 & 91.1 \small$\pm$\small1.6 & {\color{green}91.7} \small$\pm$\small1.3 & {\color{green}92.0} \small$\pm$\small1.6\\
input\_finetune & 89.7 \small$\pm$\small0.5 & {\color{green}91.1} \small$\pm$\small0.6 & {\color{green}91.4} \small$\pm$\small0.3 & {\color{green}90.2} \small$\pm$\small0.8 & {\color{green}91.1} \small$\pm$\small1.0\\
output\_finetune & 87.2 \small$\pm$\small3.3 & 86.5 \small$\pm$\small2.5 & 86.5 \small$\pm$\small2.8 & 86.1 \small$\pm$\small2.6 & 86.4 \small$\pm$\small2.4\\
classifier\_finetune & 64.5 \small$\pm$\small2.7 &  64.9 \small$\pm$\small4.6 & 64.9 \small$\pm$\small5.6 & {\color{green}66.7} \small$\pm$\small2.3 & {\color{green}66.5} \small$\pm$\small2.5 \\
\hline
prompt tune & 79.9 \small$\pm$\small7.5 & {\color{green}88.3} \small$\pm$\small5.0 & {\color{green}88.9} \small$\pm$\small4.8 & {\color{green}88.6} \small$\pm$\small3.3 & {\color{green}88.8} \small$\pm$\small3.1\\
gradient search & 86.5 \small$\pm$\small2.9 & {\color{green}89.0} \small$\pm$\small1.5 & {\color{green}88.2} \small$\pm$\small1.7 &
\end{tabular}
\label{sst2}
\end{table*}

\end{comment}

\section{Related Works}

\noindent
{\bf Prompt Engineering}:
Describe P-tuning, P-Tuning V2, Prefix-Tuning, Blax-Box Tuning, Levenstian Tuning. Chain of Thought Prompting.

The recent discrete optimization method RLPrompt \cite{deng-etal-2022-rlprompt} which uses the on-policy version of the soft Q-learning \cite{https://doi.org/10.48550/arxiv.2106.07704} to find the optimal discrete prompt tokens in a gradient-free setting.

\noindent
{\bf Paraphrase Generation}:
Our main goal in this study is not to provide a state-of-the-art system for diverse paraphrase generation. We investigate the effect of input paraphrasing on the prompt optimization techniques for efficiently querying large language models.

\section{Conclusion}

\section{Limitations}
- We are not doing the best in paraphrasing, more can be done by deploying better paraphraser models.

- Merging prompts with original inputs.

- Tested on large models only, not test on truly large models.

- Pre-trained paraphrase generator requires a supervised corpus for that particular language. More research on unsupervised paraphrase generation maybe helpful.

\section*{Ethics Statement}
Scientific work published at ACL 2023 must comply with the ACL Ethics Policy.\footnote{\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

\begin{comment}
\section*{Acknowledgements}
This document has been adapted by Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers from the style files used for earlier ACL, EMNLP and NAACL proceedings, including those for
EACL 2023 by Isabelle Augenstein and Andreas Vlachos,
EMNLP 2022 by Yue Zhang, Ryan Cotterell and Lea Frermann,
ACL 2020 by Steven Bethard, Ryan Cotterell and Rui Yan,
ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell,
ACL 2012 by Maggie Li and Michael White,
ACL 2010 by Jing-Shin Chang and Philipp Koehn,
ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
ACL 2002 by Eugene Charniak and Dekang Lin,
and earlier ACL and EACL formats written by several people, including
John Chen, Henry S. Thompson and Donald Walker.
Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.
\end{comment}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Task Instructions \& Input Format}
\label{task-instruct-input-format:appendix}
Table~\ref{task-input-format} provides a summary of the task instructions that we append before the inputs, as well as the class verbalizers that we employ for classifying the input text using generative LM tuning techniques. The instructions and input templates that we use are derived from prior work in prompt optimization~\cite{deng-etal-2022-rlprompt}.

\begin{table*}
\centering
\caption{Number of Classes $C$, the input format, and the instruction used per dataset. The label words are provided within the instructions.}
\begin{tabular}{ p{0.1\linewidth} | p{0.05\linewidth} | p{0.2\linewidth} | p{0.5\linewidth} }
\hline
Dataset & $C$ & Input Format & Instruction \\
\hline
SST2 & 2 & ``<s> \{Instruction\} \{Text\} . It was <mask> </s>'' & ``In this task, you are given sentences from movie reviews. The task is to classify a sentence as `great' if
the sentiment of the sentence is positive or as `terrible' if the sentiment of the sentence is negative.''\\
\hline
SST5 & 5 & ``<s> \{Instruction\} \{Text\} . It was <mask> </s>'' & ``In this task, you are given sentences from movie reviews. Based on the given review, classify it to one of
the five classes: (1) terrible, (2) bad, (3) okay, (4) good, and (5) great.''\\
\hline
AGâ€™s News & 4 & ``<s> \{Instruction\} <mask> News: \{text\} </s>'' & ``In this task, you are given a news article. Your task is to classify the article to one out of the four topics
`World', `Sports', `Business', `Tech' if the article's main topic is relevant to the world, sports, business, and technology, correspondingly. If you are not sure about the topic, choose the closest option.''\\
\hline
\end{tabular}
\label{task-input-format}
\end{table*}

\section{Further Training Details}
\label{training-details-extra:appendix}
In Tables~\ref{hyper-parameters} and~\ref{learning-rates}, we provide the hyper-parameters and learning rates used across all datasets.

\begin{table}[t]
\centering
\caption{Shared hyper-parameters used across all experiments and datasets.}
\begin{tabular}{ c | c }
\hline
Hyper-parameter & Value\\
\hline
Top-$k$ candidates in GS & $k$=20 \\
Top-$k$ in GrIPS & $k$=20 \\
Chain edits in GrIPS & False \\
Search level in GrIPS & Word level \\
Train batch size & 2 in GS, 8 otherwise \\
Weight decay & 0.01 \\
Max epochs & 50 \\
Input length cutoff & 128 tokens \\
Output length cutoff & 128 tokens \\
Paraphrase sample size & $M$=8 \\
Checkpointing steps & 8 \\
$\beta$ in PPO & 0.6 \\
$D^{'}$ in ClsTune & 128 \\
Prompt len in SpTune & $L$=25
\end{tabular}
\label{hyper-parameters}
\end{table}

\begin{table}[t]
\centering
\caption{Learning rates used per Language Model (LM) technique.}
\begin{tabular}{ c | c }
\hline
LM Technique & Learning Rate\\
\hline
GS & No rate \\
GrIPS & No rate \\
AlTune & 0.00001 \\
InTune &  0.001 \\
HTune & 0.001 \\
ClsTune & 0.01 \\
SpTune & 0.001 \\
LoRA & ?
\end{tabular}
\label{learning-rates}
\end{table}

\end{document}
