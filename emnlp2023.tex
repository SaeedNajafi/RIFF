% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{comment}
\usepackage[inkscapeformat=png]{svg}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Language Model-Friendly Paraphrase Search \\
for Efficient Language Model Tuning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Saeed Najafi \and
  Alona Fyshe \\
  Department of Computing Science, University of Alberta, Canada\\
  \texttt{\{snajafi,alona\}@ualberta.ca} \\}

\begin{document}
\maketitle
\begin{abstract}
Pre-trained Language Models (PLMs) have shown remarkable performance when refined for downstream text processing tasks. Recently, researchers have effectively guided PLMs to produce desired outputs for specific tasks by optimizing input prompts or adjusting a few additional parameters in the model. Nonetheless, both prompt engineering and efficient tuning studies have primarily focused on discovering suitable prompts or adaptation weights derived from the original task input text. In this study, we explore the potential impact of minor alterations to the original task input text on the ultimate performance of downstream tasks when combined with discrete and continuous prompt optimization or efficient tuning methods. Our experimentation on few-shot Natural Language Understanding (NLU) datasets reveals that enriching the training datasets with paraphrased versions of the input text significantly enhances the performance of different language model tuning techniques. To find the most effective paraphrases for enhancing the downstream language model, we apply a learning objective based on maximum-marginal likelihood (MML) estimation and off-policy sampling to tune the paraphrase model. Our findings indicate that MML combined with off-policy sampling surpasses the recently proposed proximal policy optimization method, considering different text decoding strategies such as beam search and top-p sampling.
% [Further details on the results for NLU tasks]
\end{abstract}


\section{Introduction}
Motivating points for introduction?

- Are LMs and the prompts or the adaptation matrices robust to paraphrasing inputs?

- What kind of modifications we can make?

- Drop irrelevent phrases such as time phrases!

- Change named entities!

- Change the input tense!

\section{Methods}
We focus on classification problems for Natural Language Understanding (NLU) tasks where we have access to supervised training examples $D_{\text{supp}} = \{(x_i, y_i)\}_{i=1}^{N}$. Our goal is to update the parameter set $\theta_{\text{lm}}$ of a language model by maximizing the probability of the class label $y_i$ given the input $x_i$: $P_{\theta_{\text{lm}}} (y_i | x_i)$. To augment $D_{\text{supp}}$ with semi-supervised examples, we generate $M$ paraphrases for each $x_i$ using the paraphrase generator $P_{\theta_{\text{par}}} (z_{i,j} | x_i)$, where $z_{i,j}$ represents the $j$-th paraphrase for the input $x_i$, preserving the same semantic meaning but with a different syntactic form. By incorporating these generated paraphrases, we optimize the following objective function:
\begin{multline}
J_{\theta_{\text{lm}}} := \sum_{i=1}^{N} \{\log P_{\theta_{\text{lm}}} (y_i | x_i) + \\
\frac{1}{M} \times \sum_{j=1}^{M} \log P_{\theta_{\text{lm}}} (y_i | z_{i,j})\}
\label{lmfp-augmentation-objective}
\end{multline}
\subsection{LM Tuning Techniques}
In order to train the language model and optimize Equation \ref{lmfp-augmentation-objective}, adjustments need to be made to the parameter set $\theta_{\text{lm}}$. While a naive approach would involve updating every parameter in the language model to optimize the training objective (referred to as the "All-Finetuning" or AllTune approach), this method can be computationally intensive, especially with large language models and limited computational resources. As a result, we will explore the impact of paraphrase augmentation along with seven other efficient tuning techniques~\cite{pmlr-v97-houlsby19a} or prompt engineering techniques~\cite{liu2021pretrain}.

We assume that each input $x$ or its paraphrase $z$ is preceded by task instruction $p$. The task instruction, represented by the symbol $p$ for consistency with prompt engineering literature, serves as a parameter-free, gradient-free technique for enhancing the performance of the pre-trained language model across various downstream tasks~\cite{DBLP:journals/corr/abs-2005-14165, petroni-etal-2019-language, deng-etal-2022-rlprompt}. When using only the task instructions, no parameters of the language model are updated ($\theta_{\text{lm}}=\emptyset$), and zero-shot predictions are made solely on the development data. By incorporating these task instructions into the input or paraphrases, we further investigate the following language model tuning techniques.

\textbf{Gradient-Search (GS):}
The GS technique is based on the recent AUTOPROMPT~\cite{shin-etal-2020-autoprompt} method, which allows for optimizing task instructions without updating any parameters in the model. The search process begins in the vocabulary space, approximating the change in label log-likelihood when replacing token $p_{i}$ in the task instruction with another token $v$ from the vocabulary set. In our implementation, each search iteration randomly selects only one mini-batch of training examples and then randomly selects a token from the task instruction to update. The top $k$ candidate tokens are determined based on the change in label log-likelihood, approximated using the label directional gradient in the embedding direction $w_v$ of the candidate token $v$: $Top_v \; \{w^{T}_{v} . \nabla_{w_{p_i}} \log P_{\text{lm}}(y|p,x)\}$. The resulting $k$ new task instructions are evaluated again on the same training examples\footnote{The original AUTOPROMPT evaluates the new candidate instructions on another training mini-batch. For fewshot classification, we re-use the drawn training mini-batch to evaluate the complete new candidate instructions.}, considering the label log-likelihood, and the top-performing instruction is retained for the next search iteration. This process continues with the next iteration using another training mini-batch. We also monitor the task performance on the validation set and revert to the best saved task instruction if the current search instruction does not enhance validation performance. It is worth noting that this technique finds an optimized task instruction in the discrete space, which often yields ungrammatical strings~\cite{shin-etal-2020-autoprompt}. Previous research applying this technique for prompt optimization always used the original input $x$ for searching new task prompts~\cite{shin-etal-2020-autoprompt, deng-etal-2022-rlprompt}. However, we investigate the impact of incorporating paraphrases of $x$ during search.

\textbf{GrIPS:} The Gradient-free Edit-based Instruction Search (GrIPS)~\cite{prasad-etal-2023-grips} serves as our second parameter-free method for discrete prompt optimization. Similar to the previous GS technique, GrIPS conducts a search iteration over a training mini-batch. Within each iteration, a sequence of four edit operations is performed: randomly deleting one token from the instruction, randomly adding one of the previously deleted tokens back to the instruction, randomly swapping the position of two tokens in the instruction, and finally paraphrasing one of the tokens in the instruction using a pre-trained Pegasus paraphrase model~\cite{pmlr-v119-zhang20ae}. In evaluating candidate instructions, GrIPS uses a score function based on balanced accuracy, augmented with the entropy of predictions over the training mini-batch, instead of measuring the change in label log-likelihood~\cite{prasad-etal-2023-grips}. Similar to the GS method, we monitor the task performance on the development set to retain the best instruction if the new instructions are not enhancing the performance. Optimized instructions generated by GrIPS also tend to be ungrammatical strings. It is worth noting that the paraphrase model used in GrIPS has only been applied to the instructions, not the original text $x$. Therefore, we explore whether further paraphrasing of $x$ would enhance the performance of GrIPS in discovering instructions that offer better generalization.

\textbf{Input-Finetuning (InTune):} As a straightforward and efficient tuning technique, we only update the input embedding table in the transformer architecture. This method requires gradient computation similar to All-Finetuning (AllTune) as well as the GS method. During training with InTune, $\Theta(V \times D)$ parameters are updated, where $D$ represents the dimension of the embedding vectors and $V$ denotes the vocabulary size~\footnote{Additionally, the bias and weight of the normalization layer associated with the input embedding table are also updated, but they are a function of the input embeddings.}.

\textbf{LM-Head-Finetuning (HTune):} The transformer-based pre-trained language models consist of a language modeling head, which maps the hidden vectors to the token logit (i.e., the score before the softmax function) for each token in the vocabulary. In the HTune technique, we solely update this language modeling head. This process involves updating $\Theta(D \times V)$ parameters, where $D$ represents the dimension of the hidden vectors in the final layer of the transformer architecture, and $V$ denotes the vocabulary size.

\textbf{Classifier-Finetuning (ClsTune):} In ClsTune, we first create a feature representation $h(x)$ for the input text $x$ using average pooling of the final hidden vectors in the last layer of the language model. Here, we assume that the language model (feature extractor) remains fixed, and we then construct a two-layer feedforward layer with the $gelu$ activation function~\cite{DBLP:journals/corr/HendrycksG16} as our classification module on top of the language model. The classification module is defined as follows: $y = softmax(W \times gelu(U \times h(x) + f) + b)$. In ClsTune, we only update the weight matrices $W$ and $U$, along with the bias vectors $f$ and $b$. The total number of parameters to update in ClsTune is $\Theta(D' \times D + D' + C \times D' + C)$, where $D$ represents the dimension of the hidden vectors in the language model, $D'$ corresponds to the dimension of the bias vector $f$, and $C$ indicates the number of labels.

\textbf{Softprompt-Tuning (SpTune):} In SpTune~\cite{lester-etal-2021-power}, $L$ virtual prompt tokens are prepended to the task instruction. These $L$ virtual tokens are associated with $L$ dedicated prompt embedding vectors, extending the sequence of vectors derived from the task instruction and input text with an additional $L$ trainable feature vectors. During training, the original embedding table of the transformer model remains fixed, while a new prompt embedding table is trained by backpropagating the label log-likelihood into the prompt embedding table. This method necessitates the update of $\Theta(L \times D)$ parameters, where $D$ denotes the dimension of the embedding table in the language model, and $L$ signifies the length of the soft prompts.

\textbf{Low-Rank Adaptation (LoRA):} LoRA is one of the latest efficient-tuning techniques specifically designed for large language models~\cite{DBLP:journals/corr/abs-2106-09685}. It learns low-rank adaptation matrices for the query and value weight matrices within the transformer model. For a pre-trained weight matrix $W_q \in R^{d \times k}$, LoRA learns the necessary adaptation (i.e., modification) of the weight matrix for a downstream task through a low-rank decomposition, expressed as $W_q + \triangle W_q = W_q + BA$. Here, $B \in R^{d \times r}$, $A \in R^{r \times k}$, and the rank $r \le min(d, k)$. The adaptation matrices $A$ and $B$ are the only parameters subject to training, while the original matrix $W_q$ does not receive any gradient updates. Studies have shown that LoRA performs on par with, or better than, AllTune across various large language models~\cite{DBLP:journals/corr/abs-2106-09685}.

All the Language Model (LM) tuning techniques we have discussed will use the same input format. For example in the sentiment classification task, we use the following format:
\textbf{``<s> \{instruction\} \{text\} . It was <mask> </s>''}. Except for ClsTune, all of our LM tuning techniques aim to maximize the probability of the correct label token replacing the <mask> token. In contrast, ClsTune takes the formatted input and classifies it into one of the predefined class IDs.

\subsection{LM-Friendly Paraphrase Search (LMFPS)}
\label{paraphrase-objectives}
In this section, we present the learning objectives for training our paraphrase generator. Given a training example $(x, y)$, our objective is to classify the input $x$ into the gold label $y$ by maximizing the log likelihood $\log P(y|x)$. We assume that there exist paraphrases of the input $x$ that could facilitate the prediction of the correct class. These paraphrases should retain the semantic meaning of $x$ while exhibiting syntactic differences. Our aim is to generate paraphrases $z_{j}$ based on the input $x$, enabling the downstream language model to predict the correct label $y$ with greater confidence. Consequently, our data log likelihood is factorized into the following marginalization over the space of paraphrases, where $\theta_{\text{par}}$ and $\theta_{\text{lm}}$ represent the parameters of the paraphrase generator and the downstream language model, respectively:
\begin{multline}
J_{\theta_{\text{par}}} := \log E_{z} [P(y | z)] = \\ \log \sum_{z} P_{\theta_{\text{par}}}(z | x) \times P_{\theta_{\text{lm}}}(y | z)
\label{lmfp-main-objective}
\end{multline}

To train the paraphrase generator and optimize the objective stated in Equation~\ref{lmfp-main-objective}, we explore four distinct learning aspects: a) two methods for gradient approximation, b) a reward normalization technique, c) three decoding techniques for sampling paraphrases, and d) two approaches to ensure grammatical integrity during paraphrase generation. By combining these elements, we employ various learning approaches to refine the paraphrase generator with the aid of the downstream language model. In the subsequent paragraphs, we will describe our suggested options for each aspect.

\subsubsection{Gradient Approximation}
For a given training example $(x, y)$ and its paraphrase $z$, we define the reward for $z$ as $R(z) = \log P_{\theta_{\text{lm}}} (y | z)$.

When approximating the gradient vector of objective~\ref{lmfp-main-objective} concerning $\theta_{\text{par}}$, we propose two strategies. These include using Maximum Marginal Likelihood (MML) estimation or approximating the gradient vector of the paraphrase model via the Policy Gradient (PG) theorem. Notably, gradient updates using these two methods exhibit a close relationship, with the main difference lying in the posterior coefficient utilized to score each sample~\cite{guu-etal-2017-language}. We can recast the main objective~\ref{lmfp-main-objective} into the following function representing the expected reward:
\begin{multline}
J_{\theta_{\text{par}}}
:= \log E_{z} [e^{R(z)}],
z \sim P_{\theta_{\text{par}}}(.|x)
\label{lmfp-expect-objective}
\end{multline}

Given each input $x$, if we extract paraphrase samples from $P_{\theta_{\text{par}}}(.|x)$ and approximate the expectation in $J_{\theta_{\text{par}}}$ via numerical summation, we optimize the objective using MML estimation. This process results in the following gradient update:
\begin{multline}
\nabla J^{\text{mml}}_{\theta_{\text{par}}} := \nabla_{\theta_{\text{par}}} \log E_{z} [e^{R(z)}] = \\
\sum^{M}_{j=1} \phi^{\text{mml}}(z_{j}) \times \nabla_{\theta_{\text{par}}} \log P_{\theta_{\text{par}}}(z_{j}|x) \\
\phi^{\text{mml}}(z_{j}) = \frac{P_{\theta_{\text{par}}}(z_{j}|x) \times e^{R(z_{j})}}{\sum^{M}_{j^{'}=1} P_{\theta_{\text{par}}}(z_{j^{'}}|x) \times e^{R(z_{j^{'}})}}
\label{mml-objective}
\end{multline}

By introducing the $\log$ inside the expectation (applying Jensen's inequality), we can optimize a surrogate lower bound for the objective~\ref{lmfp-expect-objective}, resulting in the following policy gradient approximation~\cite{10.5555/3009657.3009806}:
\begin{multline}
\nabla J^{\text{pg}}_{\theta_{\text{par}}} := \nabla_{\theta_{\text{par}}} E_{z} [R(z)] = \\
\sum^{M}_{j=1} \phi^{\text{pg}}(z_{j}) \times \nabla_{\theta_{\text{par}}} \log P_{\theta_{\text{par}}}(z_{j}|x) \\
\phi^{\text{pg}}(z_{j}) = P_{\theta_{\text{par}}}(z_{j}|x) \times R(z_{j})
\label{pg-objective}
\end{multline}

\subsubsection{Reward Normalization}
For our secondary learning perspective, we can either utilize the basic reward, denoted as $R(z_{j})$, or normalize the rewards among the paraphrases of a given input $x$. This process of normalization is particularly useful because it prevents the training of the paraphrase generator with rewards of varying magnitudes, which could correspond to different input examples. This situation may arise when some training pairs are easier for the language model to handle. Previous reports suggest that such normalization of rewards can significantly enhance the performance of text generators across a variety of tasks \cite{guo-etal-2022-efficient}. The normalized reward is defined as follows:
\begin{multline}
R^{n}(z_{j}) = \frac{R(z_{j}) - \mu_{j}}{\sigma_{j}}, \mu_{j} = \frac{1}{M} \sum^{M}_{j=1} R(z_{j}) \\
\sigma^{2}_{j} = \frac{1}{M} \sum^{M}_{j=1} (R(z_{j}) - \mu_{j})^2
\label{normal-reward}
\end{multline}

\subsubsection{Decoding Techniques}
To train the paraphrase generator, we employ both the MML and PG objectives as outlined in Equations \ref{mml-objective} and \ref{pg-objective}, which necessitates drawing $M$ samples from the paraphrase generator. We implement three decoding techniques for this purpose. Firstly, we utilize beam search decoding to gather these $M$ paraphrases. In order to thoroughly explore the paraphrase space, we alternatively collect the $M$ paraphrases using nucleus (top-p) sampling~\cite{holtzman2020curious}. For the top-p sampling, we establish a sampling threshold of $p=0.99$, at which we collect the minimal set of tokens from the vocabulary with a cumulative probability of at least $0.99$. We then re-sample tokens from this set. As a third option during the training phase, we blend beam search and top-p sampling. Here, we initially sample $M$ paraphrases using both methods, then combine the top $M/2$ samples from each output to construct our final $M$ samples. For data augmentation (i.e., during the test phase) in objective \ref{lmfp-augmentation-objective}, we solely use top-p sampling. As indicated by previous research, top-p sampling has been found to generate more diverse paraphrases when using our pre-trained paraphrase generator~\cite{xu-etal-2020-autoqa}.\footnote{Within our experiments, top-p sampling slightly outperformed diverse beam-search for data augmentation.}

\subsubsection{Grammatical Integrity}
In our final learning perspective, we examine if the paraphrase generator produces ungrammatical paraphrases. As we are sampling paraphrases from $P_{\theta_{\text{par}}}(z_{j}|x)$ and updating $\theta_{\text{par}}$ using these samples, the paraphrase generator may start generating ungrammatical text during this on-policy learning phase. Similar instances of degenerate generation have been reported in tasks like question generation~\cite{najafi-fyshe-2023-weakly} and program synthesis~\cite{NEURIPS2018_f4e369c0}.

To mitigate this degenerate generation, we experiment with two solutions. In line with previous work on question generation~\cite{najafi-fyshe-2023-weakly}, we consider off-policy learning for the paraphrase generator. Here, we maintain a fixed sampling module $P_{\text{fixed}}(z_{j}|x)$ for sample selection, then update the main paraphrase generator $P_{\theta_{\text{par}}}(z_{j}|x)$ within the frameworks of objectives \ref{mml-objective} and \ref{pg-objective}. Consequently, with these off-policy samples, the posterior coefficients incorporate the importance sampling ratio $s(z_{j}) = \frac{P_{\theta_{\text{par}}}(z_{j}|x)}{P_{\text{fixed}}(z_{j}|x)}$
\begin{multline}
\phi^{\text{pg}}_{\text{off}}(z_{j}) = s(z_{j}) \times R(z_{j})\\
\phi^{\text{mml}}_{\text{off}}(z_{j}) = \frac{s(z_{j}) \times e^{R(z_{j})}}{\sum^{M}_{j^{'}=1} s(z_{j^{'}}) \times e^{R(z_{j^{'}})}}
\label{off-pg-mml-objective}
\end{multline}

To avoid these degenerate paraphrases, our second solution involves imposing a penalty in the training objective if the samples drawn from the current paraphrase generator, $P_{\theta_{\text{par}}}(z|x)$, significantly deviate from those of the pre-trained paraphrase generator. We can implement this penalty as a KL-divergence penalty between the distributions of paraphrases produced by the current model and the pre-trained one. This approach resembles the Proximal Policy Optimization (PPO) with a KL penalty~\cite{DBLP:journals/corr/SchulmanWDRK17} and has been used in fine-tuning InstructGPT with a reward model trained over human feedback~\cite{ouyang2022training}. In InstructGPT's case, the reward fine-tuned model is prevented from diverging from the language model that was pre-trained on supervised data~\cite{ouyang2022training}. With this penalty in place, we define the following new objective for $\theta_{\text{par}}$:
\begin{multline}
J^{\text{ppo}}_{\theta_{\text{par}}}
:= \log E_{z} [e^{R(z)}] - \beta E_{z} [\log s(z)] \\
s(z) = \frac{P_{\theta_{\text{par}}}(z|x)}{P_{\text{fixed}}
(z|x)}, z \sim P_{\theta_{\text{par}}}(.|x)
\label{lmfp-expect-ppo-objective}
\end{multline}

Building upon the previously approximated MML and PG gradients, we can now derive the following regularized gradient vectors with respect to $\theta_{\text{par}}$. Please note that $\beta$ is a hyper-parameter in this context:
\begin{multline}
\nabla J^{\text{mml}}_{\theta_{\text{par}}} - \beta E_{z} [(\log s(z) + 1) \nabla \log P_{\theta_{\text{par}}} (z | x)] \\
\nabla J^{\text{pg}}_{\theta_{\text{par}}} - \beta E_{z} [(\log s(z) + 1) \nabla \log P_{\theta_{\text{par}}} (z | x)] \\
z \sim P_{\theta_{\text{par}}}(.|x)
\label{lmfp-expect-ppo-gradient}
\end{multline}

It's important to note that the KL penalty can be interpreted as the sum of a grammar reward, denoted by $\log P_{\text{fixed}}(z|x)$, and an entropy regularization term over $P_{\theta_{\text{par}}} (z | x)$. The entropy regularization aids in the diverse exploration of the search space~\cite{DBLP:journals/corr/MnihBMGLHSK16}, while the grammar reward discourages the learning of ungrammatical samples.

\subsection{Ensemble Inference}
\label{ensemble-inference}
After optimizing Equation \ref{lmfp-main-objective} and fine-tuning our paraphrase generator, we generate weakly-supervised examples for inclusion in the Equation \ref{lmfp-augmentation-objective} to train our downstream language model.

To predict the label of a test example, we could either use our fine-tuned language model to predict the class based on the original input $x$, or adopt an ensembling approach. For the latter, for a given $x$, we generate $M$ paraphrases using our fine-tuned paraphrase generator. We then average the prediction scores for a potential class across the $M+1$ values according to the Equation~\ref{lmfp-augmentation-objective} to predict the class for that input example $x$. This aligns with our earlier assumption that some paraphrases could be easier for the language model to predict the correct label. During the data augmentation for the language model, we select the validation set's best model according to this ensemble prediction.

\section{Experiments}

\subsection{Setup}
\subsubsection{Pre-trained Models}
We employ BART-large~\cite{lewis-etal-2020-bart}, fine-tuned on the ParaBank2 dataset over 5 million sentence-paraphrase pairs~\cite{hu-etal-2019-large}, as our pre-trained paraphrase model, $P_{\theta_{\text{par}}} (z_{i,j} | x_{i})$. These pairs were constructed by back-translating the Czech portion of an English-Czech parallel corpus~\cite{hu-etal-2019-large}. The model has been pre-trained with a token-level cross-entropy loss, calculated using the gold paraphrase output from the input sentence. We use the publicly available weights\footnote{\url{https://huggingface.co/stanford-oval/paraphraser-bart-large}} that were trained as part of the AutoQA system~\cite{xu-etal-2020-autoqa}.

\begin{comment}
Given that some of our classification experiments were conducted on the GLUE benchmark~\cite{DBLP:journals/corr/abs-1804-07461}, we opted for the BART-based model over the T5 pre-trained model. This decision was made because the GLUE dataset was part of the supervised corpora used for pre-training T5 models~\cite{DBLP:journals/corr/abs-1910-10683}. Moreover, the BART-large model, fine-tuned on the paraphrase corpus, has not encountered the input sentences and corresponding class labels of the GLUE datasets in its initial pre-training dataset.
\end{comment}

For our main language model, we utilize the RoBERTa-large model, pre-trained with the Masked Language Modeling (MLM) objective~\cite{DBLP:journals/corr/abs-1907-11692}, which has demonstrated strong performance on Natural Language Understanding (NLU) tasks. Our proposed learning framework can be readily extended to other paraphrase generators or backbone language models.

\subsubsection{Datasets}
Inspired by prior work~\cite{gao-etal-2021-making, deng-etal-2022-rlprompt}, we conduct several few-shot classification tasks as part of our experiments. These include sentiment classification tasks such as the binary sentiment dataset SST2~\cite{socher-etal-2013-recursive}, the 5-label sentiment dataset SST5~\cite{socher-etal-2013-recursive}, as well as the topic classification dataset AG's News~\cite{NIPS2015_250cf8b5}. Our experiments are carried out under the 128-shot setting, in which we randomly select 128 training examples for each unique label within the dataset. An equal number of examples are gathered to form an internal validation set. We report results on the standard development split for each dataset, with averages taken across five different models trained over five randomly generated train/validation splits. The model delivering the best performance on the validation data is used for prediction on the development set. The number of classes per dataset, as well as the instructions used for each, are outlined in Appendix~\ref{task-instruct-input-format:appendix}. Instructions and class verbalizers for the generative LM tuning techniques are based on the previous work~\cite{deng-etal-2022-rlprompt} in prompt optimization.
\subsubsection{Training \& Testing Details}
We conducted few-shot experiments using the seeds \{11, 42, 1993, 12321, 2023\}. The learning rate for each LM tuning technique was separately fine-tuned from the set \{0.5, 0.3, 0.1, 0.01, 0.001, 0.0001, 0.00001\} using the seed 11 on the SST2 dataset, and was then applied globally across other datasets and experiments. For paraphrase fine-tuning, we use the learning rate of 0.00001. Detailed information about the specific learning rates used for each LM technique, along with other hyperparameters we employed across all experiments and datasets, can be found in Appendix~\ref{training-details-extra:appendix}. For optimization, we utilized the AdamW~\cite{DBLP:journals/corr/abs-1711-05101}\footnote{\url{https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html}} optimizer with the AMSGrad variant set to True~\cite{DBLP:journals/corr/abs-1904-09237}. We implemented the methods using the HuggingFace\footnote{\url{https://huggingface.co/}} library and the PyTorch\footnote{\url{https://pytorch.org/}} machine learning framework. We report the accuracy metric on the classification datasets. The experiments were conducted using multiple NVIDIA's A40 and RTX6000 GPU cards (upto 60 GPU cards). This was facilitated by the GPU cluster provided by the Vector Institute\footnote{\url{https://vectorinstitute.ai/}}.

\subsection{Paraphrase Training}
\label{final-lmfps-result}
As discussed in Section~\ref{paraphrase-objectives}, there are four learning aspects to be considered when fine-tuning our paraphrase generator for the downstream language model. We conduct an extensive set of experiments in the 128-shot setting using the SST2 binary sentiment classification task. Our main focus is to examine the mean accuracy, which is averaged over the five validation splits. For predicting the class label for a given validation input sentence $x$, we sample $M$ paraphrases from the paraphrase generator and average the prediction scores to calculate our classification accuracy. Despite the ensembling approach described in Section~\ref{ensemble-inference}, we exclude the original input $x$ when computing the accuracy on the validation splits. This particular setting accurately captures the quality of the paraphrases for the downstream classification task. When generating paraphrases over the validation splits, we employ top-p sampling without applying any smoothing to the logits. However, for the training phase, we experiment with three distinct decoding techniques to generate paraphrase samples.

\begin{figure}[h]

\begin{center}
\includesvg[width=0.5\textwidth]{images/decoding_methods.svg}
\end{center}
\caption{The average accuracy for the 128-shot classification on the SST2 dataset is computed over five internal validation splits. We compare the performance of different text decoding techniques for both Maximum-Marginal Likelihood (MML) and Policy Gradient (PG) gradient approximations. Each data point represents an additional average across four combinations, taking into account off-policy and on-policy learning, as well as the use of basic rewards or normalized rewards.}
\label{decoding_methods}
\end{figure}

In our initial comparison, we evaluate various decoding methods, such as beam search, top-p sampling, and mixed sampling, during the training phase to generate samples. We compute the average validation curves for both Maximum-Marginal Likelihood (MML) and Policy Gradient (PG) gradient approximations, considering different learning scenarios, including on-policy and off-policy learning, and whether we use basic rewards or normalized rewards. Figure~\ref{decoding_methods} illustrates the results. We observe that beam-search decoding performs the best when using MML gradient approximation. However, on average, top-p sampling achieves the highest score with PG gradient approximations.


is between the MML and PG gradient approximations. As illustrated in Figure~\ref{pg-vs-mml-off-policy}, the MML gradient approximation outperforms the PG gradients in both off-policy and on-policy samplings (for on-policy result, please refer to Figure~\ref{pg-vs-mml-on-policy} in Appendix~\ref{training-paraphrase-extra:appendix}). One potential reason for this behavior could be the appropriate posterior coefficient assigned to the samples within the MML gradients (refer to Objective~\ref{mml-objective}).

\begin{figure}[h]
\begin{center}
\includesvg[width=0.5\textwidth]{images/mml_vs_ppo.svg}
\end{center}
\caption{Average accuracy over five internal validation splits for the 128-shot classification on the SST2 dataset using MML gradient approximations. PPO helps with the degenerate paraphrases compared to on-policy sampling.}
\label{mml-vs-ppo}
\end{figure}

Additionally, Figure~\ref{mml-vs-ppo} compares off-policy sampling with the KL penalty introduced by the PPO regularization. We initially observe that on-policy learning, after a few training steps, tends to generate degenerate paraphrases which lead to random guessing when using the binary classifier. In contrast, the KL penalty incorporated with PPO shows a higher resistance to these degenerate paraphrases. Furthermore, the MML approximation with off-policy sampling outperforms the PPO penalty as well. Off-policy sampling outperforms on-policy sampling when averaged over three decoding techniques used for drawing training samples. This holds true for both MML and PG gradient approximations (See Figure~\ref{off-policy-vs-on-policy} in Appendix~\ref{training-paraphrase-extra:appendix}).

Lastly, we evaluated and compared different decoding methods, including beam search, top-p sampling, and mixed sampling, during the training phase to generate samples. We also examined the impact of reward normalization in contrast to using the standard reward, i.e., the log-likelihood of the classifier. The corresponding validation curves for these experiments can be found in Appendix~\ref{training-paraphrase-extra:appendix}.

Upon investigating various elements of our learning objectives for fine-tuning the paraphrase generator, we find that the combination that delivers the best average performance across the validation splits includes: using MML gradient approximation, off-policy sampling, beam search for sample generation during training, and reward normalization. We name this combined approach our novel Language Model-Friendly Paraphrase Search (LMFPS) algorithm. In the subsequent experiments, we utilize LMFPS to generate paraphrases that augment the training splits while tuning the downstream language model.

\subsection{Data Augmentation with LMFPS}
some generated paraphrases before tuning, some generated paraphrases after tuning.

\begin{table*}
\centering
\caption{Average accuracy scores on the standard development sets for the SST2, SST5, and AG's News datasets for the 32-shot classification. The scores are reported for eight different language model (LM) tuning techniques, namely, AllTune, InTune, HTune, ClsTune, GS, GrIPS, SpTune, and LoRA. We conducted a comparison between two methods of generating paraphrases for data augmentation: DAug and LMFPS. DAug utilizes a simple pre-trained paraphrase model, while LMFPS, our proposed algorithm, fine-tunes the paraphrase generator using the downstream classification model. The final three rows are the average performance across the datasets. The `+ens' refers to ensemble prediction.}
%`+Es' denotes the ensemble inference method, as discussed in section \ref{ensemble-inference}.

\begin{tabular}{c | c | c | c | c | c | c | c | c}
\hline
(Dataset) & AllTune & InTune & HTune & ClsTune & GS & GrIPS & SpTune & LoRA \\
\small+Method &  &  &  &  &  &  &  &  \\
\hline
(SST2) & \small91.6 & \small85.4 & \small89.6 & \small72.7 & \small84.3 & \small85.9 & \small87.5 & \small91.7 \\
\small+DAug & \small91.9 & \small90.3 & \small89.6 & \small72.8 & \small82.2 & \small86.6 & \small85.8 & \small91.8\\
\small+LMFPS  & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\small+DAug+ens & \small92.0 & \small90.7 & \small89.9 & \small70.1 & \small79.8 & \small86.9 & \small85.8 & \small92.0\\
\small+LMFPS+ens  & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\hline
(SST5) & \small46.6 & \small43.2 & \small38.1 & \small33.8 & \small33.6 & \small00.0 & \small41.6 & \small47.8\\
\small+DAug & \small47.4 & \small42.5 & \small38.3 & \small00.0 & \small34.0 & \small00.0 & \small38.5 & \small46.4\\
\small+LMFPS & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\small+DAug+ens & \small47.8 & \small40.5 & \small38.3 & \small00.0 & \small34.2 & \small00.0 & \small38.5 & \small45.8\\
\small+LMFPS+ens  & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\hline
(AG News) & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\small+DAug & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\small+LMFPS & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\small+DAug+ens & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\small+LMFPS+ens  & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\hline
\hline
(Average) & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\small+DAug & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\small+LMFPS & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\small+DAug+ens & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\small+LMFPS+ens  & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0 & \small00.0\\
\hline
\end{tabular}
\label{lmfps-vs-dataaug-vs-main-32-shot}
\end{table*}

\section{Related Works}

\noindent
{\bf Prompt Engineering \& Efficient Tuning}: Recent research proposes various techniques for prompt optimization and efficient tuning of language models. In our experiments, we have used successful techniques from each of these areas.

FluentPrompt~\cite{shi2022human} is a recent discrete prompting technique based on the projected gradient-descent and Langevin dynamics. FluentPrompt introduces a fluency constraint within Langevin dynamics to generate a sample of high-performing prompts for more interpretable analysis of these discrete prompts. The optimized prompts by FluentPrompt performs on-par to the AutoPrompt, however they have lower perplexity~\cite{shi2022human}.

Building upon SpTune~\cite{lester-etal-2021-power} and P-tuning~\cite{li-liang-2021-prefix}, P-tuning V2~\cite{liu-etal-2022-p} introduced the concept of deep prompt tuning. This method involves injecting prompt vectors into the deeper layers of the transformer model to close the performance gap with AllTuning in medium-sized language models. We have experimented with LoRA~\cite{DBLP:journals/corr/abs-2106-09685}, a recent low-rank adaptation technique for tuning language models. Other potential methods include training bottleneck adapter modules~\cite{DBLP:journals/corr/abs-1902-00751, lin-etal-2020-exploring} added per sub-layer of the transformer model. LoRA outperforms adapter tuning and P-Tuning V2 techniques ~\cite{DBLP:journals/corr/abs-2106-09685}. The successors of LoRA include DyLoRA~\cite{valipour-etal-2023-dylora}  which dynamically learns a range of adaptation ranks, thus eliminating the need to search the rank of the adaptation matrices as a hyper-parameter. Similarly, AdaLoRA dynamically allocates the parameter budget among the weight matrices during adaptation, with matrices of higher priority (i.e., those with greater importance to the downstream task) receiving higher adaptation ranks than less important matrices~\cite{zhang2023adaptive}.

In scenarios where gradients are absent, Black-Box Tuning~\cite{DBLP:journals/corr/abs-2201-03514} applies derivative-free algorithms for optimizing continuous prompts. For discrete prompt optimization, RLPrompt~\cite{deng-etal-2022-rlprompt} employs the on-policy version of soft Q-learning~\cite{https://doi.org/10.48550/arxiv.2106.07704} to find the optimal prompt tokens in a gradient-free setting. Decoder Tuning~\cite{cui-etal-2023-decoder} learns a decoder network over the language model, thus circumventing the need for gradient computation and input-side prompt tuning in few-shot classification. In a recent study, TEMPERA~\cite{zhang2022tempera} introduced a novel approach that involves test-time discrete prompt editing using a trained RL agent. This agent is capable of modifying the instruction, in-context examples, or the verbalizers based on the given task input. The use of Language Models (LLMs) in generating instructions for downstream tasks has involved a two-step process. Initially, LLMs generate a set of candidate instructions, and subsequently, the highest-scoring instruction is utilized to prompt another LLM to perform the downstream task. This approach, known as prompt-based generation-then-filtering, has been investigated in the recent APE method~\cite{zhou2023large}. APE demonstrates the ability to generate prompts that achieve performance comparable to human-designed prompts~\cite{zhou2023large}.

To prompt language models for reasoning tasks, another line of research augment the input context with demonstration examples outlining the intermediate reasoning steps to form the answer. Providing manually or automatically generated chain-of-thoughts within these demonstrations strikingly improve LLMs performance in reasoning tasks~\cite{DBLP:journals/corr/abs-2201-11903, zhang2022automatic, NEURIPS2022_8bb0d291}.

All of the aforementioned techniques for prompt engineering and efficient tuning of the language model use the original input task (or the original input context) provided within the dataset.

\noindent
{\bf Paraphrase Generation}:
Our objective is not to present a state-of-the-art paraphrase generator, but rather to examine the impact of incorporating input paraphrases on the efficient tuning of language models. Recent advancements in generating diverse paraphrases~\cite{zhou-bhat-2021-paraphrase} have provided improved pre-trained paraphrase models, thereby these techniques can enhance performance in all our experiments as our proposed LMFPS technique can be seen as an extra fine-tuning step for the paraphrase models. These recent techniques encompass various approaches, including the use of copy mechanisms, Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Reinforcement Learning (RL) techniques\footnote{For a comprehensive overview of neural paraphrase generation, please refer to the recent survey by Zhou and Bhat~\cite{zhou-bhat-2021-paraphrase}.}.

In the following paragraphs, we provide a brief overview of similar reinforcement learning objectives employed for paraphrase generation. Li et al.~\cite{li-etal-2018-paraphrase} used a deep RL technique, training a pointer-generator network as the paraphrase generator and a decomposable attention model as the evaluator which assigns a paraphrase score to pairs of sentences. The generator was trained using the policy gradient objective, with reward shaping and scaling to stabilize the training process~\cite{li-etal-2018-paraphrase}. Another approach by Qian et al.~\cite{qian-etal-2019-exploring} focused on generating diverse paraphrases by training multiple generators, accompanied by a paraphrase discriminator and a generator discriminator. Policy gradient objective and self-critical learning~\cite{DBLP:journals/corr/RennieMMRG16} were employed for training the generators, with the baseline reward used in the policy gradient objective being the reward obtained from the greedy-decoded sequence. Liu et al.~\cite{liu-etal-2020-learning} also applied the policy gradient objective with self-critical learning, incorporating multiple reward functions such as Rouge score with the reference paraphrase, negative Rouge score with the input sentence to encourage lexical variations, and semantic similarity score between the paraphrase and the input sentence to ensure semantic fidelity.

Another study by Du and Ji~\cite{du-ji-2019-empirical} compared the use of imitation learning algorithm DAGGER with policy gradient REINFORCE for paraphrase generation. The policy gradient objective has also been applied in generating paraphrases while considering multiple objectives for entailment relation-aware paraphrase generation~\cite{Sancheti_Srinivasan_Rudinger_2022}. In the context of chatbot responses, a recent work studies unsupervised paraphrase generation with proximal policy optimization, aiming to maximize a combination of rewards such as textual entailment, semantic similarity, language fluency, and lexical dissimilarity~\cite{DBLP:journals/corr/abs-2103-12777}. Similarly, the policy gradient objective has been employed to optimize multiple rewards, similar to previous work, for unsupervised paraphrase generation~\cite{10.1145/3394486.3403231}.

While previous studies have applied RL techniques for paraphrase generation, we propose the use of Maximum-Marginal Likelihood (MML) gradients instead of policy gradients to train our paraphrase model. Additionally, we adopt an off-policy setting to ensure that the generated paraphrases stay grammatical.
\noindent
{\bf Data Augmentation}:

\section{Conclusion}


\section{Limitations}
- We are not doing the best in paraphrasing, more can be done by deploying better paraphraser models.

- Merging prompts with original inputs.

- Tested on large models only, not test on truly large models.

- Pre-trained paraphrase generator requires a supervised corpus for that particular language. More research on unsupervised paraphrase generation maybe helpful.

\section*{Ethics Statement}
Scientific work published at ACL 2023 must comply with the ACL Ethics Policy.\footnote{\url{https://www.aclweb.org/portal/content/acl-code-ethics}} We encourage all authors to include an explicit ethics statement on the broader impact of the work, or other ethical considerations after the conclusion but before the references. The ethics statement will not count toward the page limit (8 pages for long, 4 pages for short papers).

\begin{comment}
\section*{Acknowledgements}
This document has been adapted by Jordan Boyd-Graber, Naoaki Okazaki, Anna Rogers from the style files used for earlier ACL, EMNLP and NAACL proceedings, including those for
EACL 2023 by Isabelle Augenstein and Andreas Vlachos,
EMNLP 2022 by Yue Zhang, Ryan Cotterell and Lea Frermann,
ACL 2020 by Steven Bethard, Ryan Cotterell and Rui Yan,
ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
ACL 2017 by Dan Gildea and Min-Yen Kan, NAACL 2017 by Margaret Mitchell,
ACL 2012 by Maggie Li and Michael White,
ACL 2010 by Jing-Shin Chang and Philipp Koehn,
ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
ACL 2002 by Eugene Charniak and Dekang Lin,
and earlier ACL and EACL formats written by several people, including
John Chen, Henry S. Thompson and Donald Walker.
Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.
\end{comment}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Task Instructions \& Input Format}
\label{task-instruct-input-format:appendix}
Table~\ref{task-input-format} provides a summary of the task instructions that we append before the inputs, as well as the class verbalizers that we employ for classifying the input text using generative LM tuning techniques. The instructions and input templates that we use are derived from prior work in prompt optimization~\cite{deng-etal-2022-rlprompt}.

\begin{table*}
\centering
\caption{Number of Classes $C$, the input format, and the instruction used per dataset. The label words are provided within the instructions.}
\begin{tabular}{ p{0.1\linewidth} | p{0.05\linewidth} | p{0.2\linewidth} | p{0.5\linewidth} }
\hline
Dataset & $C$ & Input Format & Instruction \\
\hline
SST2 & 2 & ``<s> \{Instruction\} \{Text\} . It was <mask> </s>'' & ``In this task, you are given sentences from movie reviews. The task is to classify a sentence as `great' if
the sentiment of the sentence is positive or as `terrible' if the sentiment of the sentence is negative.''\\
\hline
SST5 & 5 & ``<s> \{Instruction\} \{Text\} . It was <mask> </s>'' & ``In this task, you are given sentences from movie reviews. Based on the given review, classify it to one of
the five classes: (1) terrible, (2) bad, (3) okay, (4) good, and (5) great.''\\
\hline
AG’s News & 4 & ``<s> \{Instruction\} <mask> News: \{text\} </s>'' & ``In this task, you are given a news article. Your task is to classify the article to one out of the four topics
`World', `Sports', `Business', `Tech' if the article's main topic is relevant to the world, sports, business, and technology, correspondingly. If you are not sure about the topic, choose the closest option.''\\
\hline
\end{tabular}
\label{task-input-format}
\end{table*}

\section{Further Training Details}
\label{training-details-extra:appendix}
In Tables~\ref{hyper-parameters} and~\ref{learning-rates}, we provide the hyper-parameters and learning rates used across all datasets.

\begin{table}[t]
\centering
\caption{Shared hyper-parameters used across all experiments and datasets.}
\begin{tabular}{ c | c }
\hline
Hyper-parameter & Value\\
\hline
Top-$k$ candidates in GS & $k$=8 \\
Top-$k$ in GrIPS & $k$=8 \\
Chain edits in GrIPS & False \\
Search level in GrIPS & Word level \\
Train batch size & 4 \\
Weight decay & 0.0001 \\
SST2 Max epochs & 100 \\
SST5 Max epochs & 50 \\
Input length cutoff & 128 tokens \\
Output length cutoff & 128 tokens \\
Paraphrase sample size & $M$=8 \\
Checkpointing steps & 8 \\
$\beta$ in PPO & ? \\
$D^{'}$ in ClsTune & 128 \\
Prompt len in SpTune & $L$=25 \\
Lora $\alpha$ & 32 \\
Lora $r$ & 8 \\
Lora dropout & 0.1 \\
\end{tabular}
\label{hyper-parameters}
\end{table}

\begin{table}[t]
\centering
\caption{Learning rates used per Language Model (LM) tuning technique.}
\begin{tabular}{ c | c }
\hline
LM Tuning Technique & Learning Rate\\
\hline
GS & No rate \\
GrIPS & No rate \\
AllTune & 0.00001 \\
InTune &  0.001 \\
HTune & 0.00001 \\
ClsTune & 0.001 \\
SpTune & 0.001 \\
LoRA & 0.0001
\end{tabular}
\label{learning-rates}
\end{table}

\section{Paraphrase Training (Further Results)}
\label{training-paraphrase-extra:appendix}

This section provides additional results that compare our training objectives for the fine-tuning of the paraphrase generator using the downstream language model. The supplementary experiment shown in Figure~\ref{pg-vs-mml-on-policy} confirms that the MML gradient approximation surpasses the PG estimation within the on-policy samplings.

\begin{figure}[h]
\begin{center}
\includesvg[width=0.5\textwidth]{images/mml_vs_pg_on_policy.svg}
\end{center}
\caption{Average accuracy over five internal validation splits for the 128-shot classification on the SST2 dataset using on-policy sampling with MML and PG gradient approximations.}
\label{pg-vs-mml-on-policy}
\end{figure}

Additionally, Figure~\ref{off-policy-vs-on-policy} demonstrates that off-policy sampling outperforms on-policy sampling when averaged over three decoding techniques used during training phase: beam search, top-p, and mixed sampling. This trend holds true for both MML and PG gradient approximations. In the case of on-policy sampling, it tends to generate degenerate, ungrammatical paraphrases after a few training steps.

\begin{figure}[h]
\begin{center}
\includesvg[width=0.5\textwidth]{images/on_policy_vs_off_policy.svg}
\end{center}
\caption{Average accuracy over five internal validation splits for the 128-shot classification on the SST2 dataset. Performance is further averaged across three text decoding techniques used during training phase for both MML and PG gradient approximations.}
\label{off-policy-vs-on-policy}
\end{figure}

Our training phase utilizes three decoding techniques for generating paraphrase samples: beam-search, top-p sampling, and mixed sampling. The average accuracy across the validation splits for each of these decoding techniques is plotted in Figure~\ref{beam-search-top-mixed}. These curves represent further averages across four combinations from both MML and PG gradient approximations, as well as on-policy and off-policy sampling. Our results indicate that mixed sampling tends to exhibit greater robustness to degenerate paraphrases than beam-search decoding during the later stages of training.

\begin{figure}[h]
\begin{center}
\includesvg[width=0.5\textwidth]{images/topp_vs_beam_mixed.svg}
\end{center}
\caption{Average accuracy over five internal validation splits for the 128-shot classification on the SST2 dataset. Performance is further averaged across MML and PG gradient approximations as well as on-policy and off-policy sampling.}
\label{beam-search-top-mixed}
\end{figure}

In a final attempt to gauge the influence of reward normalization, we deploy this technique for both MML and PG gradient approximations, utilizing beam-search decoding during the generation of samples. The resultant average accuracy on the SST2 validation splits is then plotted.

In the case of on-policy learning, as illustrated in Figure~\ref{zscore-on-policy}, reward normalization leads to enhanced robustness against degenerate paraphrase generation, irrespective of whether MML or PG gradient estimation is employed.

Interestingly, when employing off-policy sampling, we don't observe a significant disparity between the usage of basic rewards or reward normalization. This observation is visually represented in Figure~\ref{zscore-off-policy}.

\begin{figure}[h]
\begin{center}
\includesvg[width=0.5\textwidth]{images/zscore_on_policy.svg}
\end{center}
\caption{The average accuracy over five internal validation splits for the 128-shot classification on the SST2 dataset is plotted. Reward normalization (z\_score) assists in mitigating the generation of degenerate paraphrases in both MML and PG gradient estimations.}
\label{zscore-on-policy}
\end{figure}

\begin{figure}[h]
\begin{center}
\includesvg[width=0.5\textwidth]{images/zscore_off_policy.svg}
\end{center}
\caption{The average accuracy over five internal validation splits for the 128-shot classification on the SST2 dataset is plotted. Reward normalization (z\_score) does not result in significant variations with both MML and PG gradient estimations.}
\label{zscore-off-policy}
\end{figure}

\section{LMFPS vs. Simple Augmentation}
\label{lmfps-simple-augmentation:appendix}
The proposed LMFPS objective in Section~\ref{final-lmfps-result} fine-tunes the paraphrase generator using the downstream classification model. In this section, we aim to juxtapose our LMFPS algorithm with a basic data augmentation method that leverages the pre-trained paraphrase generator without any fine-tuning based on the objective~\ref{lmfp-main-objective}. The settings used to predict classes on the final development sets are identical for both LMFPS and the simple data augmentation method, which we refer to as DAug. These settings include ensemble inference and the application of top-p sampling without logit smoothing for generating the paraphrases. Both LMPFS and DAug use the same objective to train the language model, as detailed in Objective~\ref{lmfp-augmentation-objective}.

\end{document}
